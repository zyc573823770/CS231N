{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "### What is PyTorch?\n",
    "It is a **replacement for NumPy** to use the power of GPUs, and a **deep learning research platform** that provides maximum flexibility and speed ([source](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)).\n",
    "\n",
    "You can create a [PyTorch tensor](https://pytorch.org/docs/stable/tensors.html) in a similary way that you create a NumPy ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch version:1.4.0\n\nCreate a zero ndarray in NumPy:\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nCreate a zero tensor in PyTorch:\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(\"torch version:\", torch.__version__) # please use pytorch 1.0\n",
    "\n",
    "print('\\nCreate a zero ndarray in NumPy:')\n",
    "zero_np = np.zeros([2, 3])\n",
    "print(zero_np)\n",
    "print('\\nCreate a zero tensor in PyTorch:')\n",
    "zero_pt = torch.zeros([2,3])\n",
    "print(zero_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index into the tensor the same way you index a ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "numpy: zero_np[0,1]: 0.0\t(type: <class 'numpy.float64'>)\ntorch: zero_pt[0,1]: tensor(0.)\t(type: <class 'torch.Tensor'> / shape: torch.Size([]))\n       zero_pt[0,1].item(): 0.0\t(type: <class 'float'>)\n"
    }
   ],
   "source": [
    "print(\"numpy: zero_np[0,1]: {}\\t(type: {})\".format(str(zero_np[0,1]), type(zero_np[0,1])))\n",
    "print(\"torch: zero_pt[0,1]: {}\\t(type: {} / shape: {})\".format(str(zero_pt[0,1]), type(zero_pt[0,1]), zero_pt[0,1].shape))\n",
    "# Use \"item()\" to get a Python number from a single-valued tensor.\n",
    "print(\"       zero_pt[0,1].item(): {}\\t(type: {})\".format(zero_pt[0,1].item(), type(zero_pt[0,1].item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ndarray can be turned into a tensor, and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Turn a ndarray into a tensor with \"torch.tensor()\":\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\nor \"torch.from_numpy():\"\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)\n\nTurn a tensor into ndarray with \".numpy()\":\n[[0. 0. 0.]\n [0. 0. 0.]]\n<class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "print('Turn a ndarray into a tensor with \"torch.tensor()\":')\n",
    "zero_pt_from_np = torch.tensor(zero_np)\n",
    "print(zero_pt_from_np)\n",
    "print('or \"torch.from_numpy():\"')\n",
    "zero_pt_from_np = torch.from_numpy(zero_np)\n",
    "print(zero_pt_from_np)\n",
    "\n",
    "print('\\nTurn a tensor into ndarray with \".numpy()\":')\n",
    "zero_np_from_pt = zero_pt.numpy()\n",
    "print(zero_np_from_pt)\n",
    "print(type(zero_np_from_pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of PyTorch allows it to better utilize GPUs. Upon creation, a PyTorch tensor resides on the CPU. You can move a tensor across devices using `.to()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initial device:\t'cpu'\n"
    }
   ],
   "source": [
    "t = torch.randn(2)\n",
    "print(\"Initial device:\\t'{}'\".format(t.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Move to gpu:\t'cuda:0'\n"
    }
   ],
   "source": [
    "t = t.to('cuda:0')\n",
    "print(\"Move to gpu:\\t'{}'\".format(t.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Back to cpu:\t'cpu'\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-0.97502095,  0.9013278 ], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "t = t.to('cpu')\n",
    "print(\"Back to cpu:\\t'{}'\".format(t.device))\n",
    "# Why bother?\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training an MNIST Classifier\n",
    "=====\n",
    "## Custom Dataset, Model Checkpointing, and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn            # containing various building blocks for your neural networks\n",
    "import torch.optim as optim      # implementing various optimization algorithms\n",
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "import torchvision\n",
    "# transforms: transformations useful for image processing\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset\n",
    "PyTorch has many built-in datasets such as MNIST and CIFAR. In this tutorial, we demonstrate how to write your own dataset by implementing a custom MNIST dataset class. Use [this link](https://github.com/myleott/mnist_png/blob/master/mnist_png.tar.gz?raw=true) to download the mnist png dataset.\n",
    "\n",
    "If you are on GCloud, you can run these commands:\n",
    "\n",
    "`wget https://github.com/myleott/mnist_png/blob/master/mnist_png.tar.gz?raw=true`\n",
    "\n",
    "`mv mnist_png.tar.gz?raw=true mnist_png.tar.gz`\n",
    "\n",
    "`tar -xzf mnist_png.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 preload=False):\n",
    "        \"\"\" Intialize the MNIST dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # read filenames\n",
    "        for i in range(10):\n",
    "            filenames = glob.glob(osp.join(root, str(i), '*.png'))\n",
    "            for fn in filenames:\n",
    "                self.filenames.append((fn, i)) # (filename, label) pair\n",
    "                \n",
    "        # if preload dataset into memory\n",
    "        if preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for image_fn, label in self.filenames:            \n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(image.copy())\n",
    "            # avoid too many opened files bug\n",
    "            image.close()\n",
    "            self.labels.append(label)\n",
    "\n",
    "    # probably the most important to customize.\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MNIST dataset. \n",
    "# transforms.ToTensor() automatically converts PIL images to\n",
    "# torch tensors with range [0, 1]\n",
    "trainset = MNIST(\n",
    "    root='mnist_png/training',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "# We want the dataset to be shuffled during training.\n",
    "trainset_loader = DataLoader(trainset, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load the testset\n",
    "testset = MNIST(\n",
    "    root='mnist_png/testing',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "60000\n10000\n<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x000001FB4EFB1C88>\n"
    }
   ],
   "source": [
    "print(len(trainset)) # len = 60000\n",
    "print(len(testset))  # len = 10000\n",
    "print(iter(trainset_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10, 1, 28, 28])\ntorch.Size([3, 62, 242])\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.693344pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.693344\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.693344 \r\nL 251.565 248.693344 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.815219 \r\nL 244.365 224.815219 \r\nL 244.365 7.375219 \r\nL 26.925 7.375219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pddee9014df)\">\r\n    <image height=\"218\" id=\"imageedb51b9213\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAABhxJREFUeJzt3U2ITu0Dx/F70JSZ8hIrRUlWCGEvioWiSdSwt6JEkVnYeUmRIgsSTcLGaxZSMgsKEUo20khs2NiMBsWz+P+31+XpzNy/Gc98Pttf58y1+Tp1bnNPR6vV+t0C2mrSWB8AJgKhQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgYMpYH+BvMGfOnOK2c+fO4rZ///52HKd16NCh4nb69OnqtZ8/fx7t4/AveKJBgNAgQGgQIDQIEBoECA0ChAYBHa1W6/dYH2Ks9fX1Vfe9e/cWtxkzZoz2cUbk6dOn1f3IkSPF7fbt26N9HP7PEw0ChAYBQoMAoUGA0CBAaBAwYV7v9/T0FLfLly9Xr+3s7Bzt44yZ4eHh4nbw4MHidvz48XYcZ8LwRIMAoUGA0CBAaBAgNAgQGgRMmNf7jx8/Lm6rVq1qfN8nT54UtwMHDjS+b03tVfvy5csb3/f79+/F7eHDh8Vt3bp1jX9mzaJFi4pbd3d39drBwcHi9uXLl8ZnasoTDQKEBgFCgwChQYDQIEBoEDBhXu/X/rjDrFmzqte+ffu2uK1du7a4ffr06c8Ha2DevHnF7fr169Vrm77+//XrV3G7ceNGo3v+yZo1a4rbzJkzq9c+e/asuB0+fLi43bp1688Ha8ATDQKEBgFCgwChQYDQIEBoECA0CPhP/bH41atXF7eurq7G97106VJxa9dnZTUfPnwobvv27atee/fu3eI2efLk4jZpUvnf5M2bN1d/5lhYuXJlcVu4cGHwJP/jiQYBQoMAoUGA0CBAaBAgNAj4T73eHxgYKG7fvn0rblOnTq3ed8eOHcWt9mspb968qd63qfXr1xe3BQsWVK+tfTQwf/78xmdqh/fv3xe3ixcvNr7vo0ePGl/blCcaBAgNAoQGAUKDAKFBgNAgYMJ8C1btm5o2btzY+L4/f/4sbrVvjhqJKVPKn8rU/pd9q9VqdXR0jPZx/uj169fF7dSpU8Wtv7+/uP348WNEZ0rzRIMAoUGA0CBAaBAgNAgQGgRMmNf7tb95vGXLluq158+fH+3jjEsvX74sbsuWLWt836GhoeK2ePHi4lb7TYO/jScaBAgNAoQGAUKDAKFBgNAgQGgQMGE+R6uZPn16dT927Fhxa9c3R/X19RW3JUuWFLfe3t7qfc+dO1fcXr16Vdzu3btX3ObOnVv9mTW1X5PZvXt34/uON55oECA0CBAaBAgNAoQGAUKDAK/3+VdOnjxZ3Hbt2tX4vteuXStuW7dubXzf8cYTDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQcCUsT7A366/v7+4PXjwoLhduHChHcdhnPJEgwChQYDQIEBoECA0CBAaBHi9P0JdXV3F7ejRo8Wtr6+vet/a329+8eLFnw/GuOKJBgFCgwChQYDQIEBoECA0CBAaBPgcbYQGBweLW09PT3GbPXt29b537twpbjdv3ixuV69erd63pre3t7ht2rSp8X1rzpw505b7jjeeaBAgNAgQGgQIDQKEBgFCg4COVqv1e6wP8Tfr7u4ubhs2bChuV65cacdxxqWvX78Wt6VLlxa3jx8/tuM4Y8ITDQKEBgFCgwChQYDQIEBoEOD1fht1dnYWt+3bt1evPXHiRHGbNm1a4zM1NTQ0VNyGh4er19a+0WtgYKDpkf4qnmgQIDQIEBoECA0ChAYBQoMAr/fHqRUrVhS3PXv2FLdt27ZV73v27Nni9u7du+J2//794vb8+fPqz8QTDSKEBgFCgwChQYDQIEBoECA0CPA5GgR4okGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoEPAPJq/osVNkcr0AAAAASUVORK5CYII=\" y=\"-6.815219\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf05ad33a9e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.549\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.36775 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.789\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(63.60775 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.029\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(96.6665 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.269\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(132.9065 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.509\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(169.1465 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.749\" xlink:href=\"#mf05ad33a9e\" y=\"224.815219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(205.3865 239.413656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb683005310\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"47.239219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 51.038437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"83.479219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 87.278437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"119.719219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 123.518437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"155.959219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 159.758437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mb683005310\" y=\"192.199219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 195.998437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.815219 \r\nL 26.925 7.375219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.815219 \r\nL 244.365 7.375219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.815219 \r\nL 244.365 224.815219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.375219 \r\nL 244.365 7.375219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pddee9014df\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.375219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"119.960815pt\" version=\"1.1\" viewBox=\"0 0 368.925 119.960815\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 119.960815 \r\nL 368.925 119.960815 \r\nL 368.925 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 96.08269 \r\nL 361.725 96.08269 \r\nL 361.725 10.307483 \r\nL 26.925 10.307483 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pfa89700719)\">\r\n    <image height=\"86\" id=\"image12a1c8d192\" transform=\"scale(1 -1)translate(0 -86)\" width=\"335\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-10.08269\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mdbf3abf3d8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.616736\" xlink:href=\"#mdbf3abf3d8\" y=\"96.08269\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(24.435486 110.681127)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.790289\" xlink:href=\"#mdbf3abf3d8\" y=\"96.08269\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(90.427789 110.681127)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.963843\" xlink:href=\"#mdbf3abf3d8\" y=\"96.08269\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(156.420093 110.681127)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.137397\" xlink:href=\"#mdbf3abf3d8\" y=\"96.08269\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(225.593647 110.681127)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.31095\" xlink:href=\"#mdbf3abf3d8\" y=\"96.08269\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(294.7672 110.681127)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma291e4988d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma291e4988d\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma291e4988d\" y=\"38.66864\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 42.467859)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma291e4988d\" y=\"66.338062\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 70.13728)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma291e4988d\" y=\"94.007483\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 97.806702)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 96.08269 \r\nL 26.925 10.307483 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 96.08269 \r\nL 361.725 10.307483 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 96.08269 \r\nL 361.725 96.08269 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 10.307483 \r\nL 361.725 10.307483 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pfa89700719\">\r\n   <rect height=\"85.775207\" width=\"334.8\" x=\"26.925\" y=\"10.307483\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    print(img.shape)\n",
    "    plt.imshow(np.transpose(img[:,:30,:30],(1,2,0)))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis = False\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda\n"
    }
   ],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Conv Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "        #        dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        # Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "        # MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        # ReLU(inplace=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: the following two ways for max pooling / relu are equivalent.\n",
    "        # 1) with torch.nn.functional:\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # 2) with torch.nn:\n",
    "        x = self.relu(self.max_pool(self.conv2_drop(self.conv2(x))))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(epoch, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            # bring data to the computing device, e.g. GPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # compute loss: negative log-likelihood\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # backward pass\n",
    "            # clear the gradients of all tensors being optimized.\n",
    "            optimizer.zero_grad()\n",
    "            # accumulate (i.e. add) the gradients from this forward pass\n",
    "            loss.backward()\n",
    "            # performs a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "            \n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "        test() # evaluate at the end of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testset_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testset_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testset_loader.dataset),\n",
    "        100. * correct / len(testset_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.280468\nTrain Epoch: 0 [1000/60000 (2%)]\tLoss: 2.301524\nTrain Epoch: 0 [2000/60000 (3%)]\tLoss: 2.319705\nTrain Epoch: 0 [3000/60000 (5%)]\tLoss: 2.298132\nTrain Epoch: 0 [4000/60000 (7%)]\tLoss: 2.194443\nTrain Epoch: 0 [5000/60000 (8%)]\tLoss: 2.137386\nTrain Epoch: 0 [6000/60000 (10%)]\tLoss: 1.924183\nTrain Epoch: 0 [7000/60000 (12%)]\tLoss: 1.496806\nTrain Epoch: 0 [8000/60000 (13%)]\tLoss: 1.530975\nTrain Epoch: 0 [9000/60000 (15%)]\tLoss: 1.183270\nTrain Epoch: 0 [10000/60000 (17%)]\tLoss: 1.214899\nTrain Epoch: 0 [11000/60000 (18%)]\tLoss: 0.839586\nTrain Epoch: 0 [12000/60000 (20%)]\tLoss: 1.256978\nTrain Epoch: 0 [13000/60000 (22%)]\tLoss: 1.271300\nTrain Epoch: 0 [14000/60000 (23%)]\tLoss: 0.424302\nTrain Epoch: 0 [15000/60000 (25%)]\tLoss: 0.857025\nTrain Epoch: 0 [16000/60000 (27%)]\tLoss: 0.297462\nTrain Epoch: 0 [17000/60000 (28%)]\tLoss: 0.673704\nTrain Epoch: 0 [18000/60000 (30%)]\tLoss: 0.802798\nTrain Epoch: 0 [19000/60000 (32%)]\tLoss: 0.618214\nTrain Epoch: 0 [20000/60000 (33%)]\tLoss: 0.295605\nTrain Epoch: 0 [21000/60000 (35%)]\tLoss: 0.424496\nTrain Epoch: 0 [22000/60000 (37%)]\tLoss: 0.654564\nTrain Epoch: 0 [23000/60000 (38%)]\tLoss: 0.546598\nTrain Epoch: 0 [24000/60000 (40%)]\tLoss: 0.558513\nTrain Epoch: 0 [25000/60000 (42%)]\tLoss: 0.549644\nTrain Epoch: 0 [26000/60000 (43%)]\tLoss: 0.741328\nTrain Epoch: 0 [27000/60000 (45%)]\tLoss: 0.320243\nTrain Epoch: 0 [28000/60000 (47%)]\tLoss: 0.605833\nTrain Epoch: 0 [29000/60000 (48%)]\tLoss: 0.760837\nTrain Epoch: 0 [30000/60000 (50%)]\tLoss: 0.631580\nTrain Epoch: 0 [31000/60000 (52%)]\tLoss: 0.215001\nTrain Epoch: 0 [32000/60000 (53%)]\tLoss: 0.102673\nTrain Epoch: 0 [33000/60000 (55%)]\tLoss: 0.934280\nTrain Epoch: 0 [34000/60000 (57%)]\tLoss: 0.632008\nTrain Epoch: 0 [35000/60000 (58%)]\tLoss: 0.276647\nTrain Epoch: 0 [36000/60000 (60%)]\tLoss: 0.415689\nTrain Epoch: 0 [37000/60000 (62%)]\tLoss: 0.622402\nTrain Epoch: 0 [38000/60000 (63%)]\tLoss: 0.383733\nTrain Epoch: 0 [39000/60000 (65%)]\tLoss: 0.326104\nTrain Epoch: 0 [40000/60000 (67%)]\tLoss: 0.759472\nTrain Epoch: 0 [41000/60000 (68%)]\tLoss: 0.417783\nTrain Epoch: 0 [42000/60000 (70%)]\tLoss: 0.398364\nTrain Epoch: 0 [43000/60000 (72%)]\tLoss: 0.087184\nTrain Epoch: 0 [44000/60000 (73%)]\tLoss: 0.161568\nTrain Epoch: 0 [45000/60000 (75%)]\tLoss: 0.785145\nTrain Epoch: 0 [46000/60000 (77%)]\tLoss: 1.200075\nTrain Epoch: 0 [47000/60000 (78%)]\tLoss: 0.485481\nTrain Epoch: 0 [48000/60000 (80%)]\tLoss: 1.097314\nTrain Epoch: 0 [49000/60000 (82%)]\tLoss: 0.238683\nTrain Epoch: 0 [50000/60000 (83%)]\tLoss: 0.809333\nTrain Epoch: 0 [51000/60000 (85%)]\tLoss: 0.614570\nTrain Epoch: 0 [52000/60000 (87%)]\tLoss: 1.108032\nTrain Epoch: 0 [53000/60000 (88%)]\tLoss: 0.344660\nTrain Epoch: 0 [54000/60000 (90%)]\tLoss: 0.858120\nTrain Epoch: 0 [55000/60000 (92%)]\tLoss: 0.106945\nTrain Epoch: 0 [56000/60000 (93%)]\tLoss: 0.550302\nTrain Epoch: 0 [57000/60000 (95%)]\tLoss: 0.463283\nTrain Epoch: 0 [58000/60000 (97%)]\tLoss: 0.595188\nTrain Epoch: 0 [59000/60000 (98%)]\tLoss: 0.352483\n43.59s\n\nTest set: Average loss: 0.1474, Accuracy: 9527/10000 (95%)\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 0.131206\nTrain Epoch: 1 [1000/60000 (2%)]\tLoss: 0.089724\nTrain Epoch: 1 [2000/60000 (3%)]\tLoss: 0.004288\nTrain Epoch: 1 [3000/60000 (5%)]\tLoss: 0.228200\nTrain Epoch: 1 [4000/60000 (7%)]\tLoss: 0.127019\nTrain Epoch: 1 [5000/60000 (8%)]\tLoss: 0.063362\nTrain Epoch: 1 [6000/60000 (10%)]\tLoss: 0.265906\nTrain Epoch: 1 [7000/60000 (12%)]\tLoss: 0.009328\nTrain Epoch: 1 [8000/60000 (13%)]\tLoss: 0.014265\nTrain Epoch: 1 [9000/60000 (15%)]\tLoss: 0.009497\nTrain Epoch: 1 [10000/60000 (17%)]\tLoss: 0.058150\nTrain Epoch: 1 [11000/60000 (18%)]\tLoss: 0.567804\nTrain Epoch: 1 [12000/60000 (20%)]\tLoss: 0.433261\nTrain Epoch: 1 [13000/60000 (22%)]\tLoss: 0.134682\nTrain Epoch: 1 [14000/60000 (23%)]\tLoss: 0.177520\nTrain Epoch: 1 [15000/60000 (25%)]\tLoss: 0.009735\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 0.114181\nTrain Epoch: 1 [17000/60000 (28%)]\tLoss: 0.085271\nTrain Epoch: 1 [18000/60000 (30%)]\tLoss: 0.070243\nTrain Epoch: 1 [19000/60000 (32%)]\tLoss: 0.005297\nTrain Epoch: 1 [20000/60000 (33%)]\tLoss: 0.104628\nTrain Epoch: 1 [21000/60000 (35%)]\tLoss: 0.039340\nTrain Epoch: 1 [22000/60000 (37%)]\tLoss: 0.001491\nTrain Epoch: 1 [23000/60000 (38%)]\tLoss: 0.005693\nTrain Epoch: 1 [24000/60000 (40%)]\tLoss: 0.028093\nTrain Epoch: 1 [25000/60000 (42%)]\tLoss: 0.005188\nTrain Epoch: 1 [26000/60000 (43%)]\tLoss: 0.142514\nTrain Epoch: 1 [27000/60000 (45%)]\tLoss: 0.019431\nTrain Epoch: 1 [28000/60000 (47%)]\tLoss: 0.031617\nTrain Epoch: 1 [29000/60000 (48%)]\tLoss: 0.001794\nTrain Epoch: 1 [30000/60000 (50%)]\tLoss: 0.009662\nTrain Epoch: 1 [31000/60000 (52%)]\tLoss: 0.044581\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.003083\nTrain Epoch: 1 [33000/60000 (55%)]\tLoss: 0.183165\nTrain Epoch: 1 [34000/60000 (57%)]\tLoss: 0.018924\nTrain Epoch: 1 [35000/60000 (58%)]\tLoss: 0.018420\nTrain Epoch: 1 [36000/60000 (60%)]\tLoss: 0.148568\nTrain Epoch: 1 [37000/60000 (62%)]\tLoss: 0.014559\nTrain Epoch: 1 [38000/60000 (63%)]\tLoss: 0.000345\nTrain Epoch: 1 [39000/60000 (65%)]\tLoss: 0.274805\nTrain Epoch: 1 [40000/60000 (67%)]\tLoss: 0.068015\nTrain Epoch: 1 [41000/60000 (68%)]\tLoss: 0.163201\nTrain Epoch: 1 [42000/60000 (70%)]\tLoss: 0.006609\nTrain Epoch: 1 [43000/60000 (72%)]\tLoss: 0.081038\nTrain Epoch: 1 [44000/60000 (73%)]\tLoss: 0.066475\nTrain Epoch: 1 [45000/60000 (75%)]\tLoss: 0.039361\nTrain Epoch: 1 [46000/60000 (77%)]\tLoss: 0.004039\nTrain Epoch: 1 [47000/60000 (78%)]\tLoss: 0.013120\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.064387\nTrain Epoch: 1 [49000/60000 (82%)]\tLoss: 0.002198\nTrain Epoch: 1 [50000/60000 (83%)]\tLoss: 0.062261\nTrain Epoch: 1 [51000/60000 (85%)]\tLoss: 0.313230\nTrain Epoch: 1 [52000/60000 (87%)]\tLoss: 0.152491\nTrain Epoch: 1 [53000/60000 (88%)]\tLoss: 0.004206\nTrain Epoch: 1 [54000/60000 (90%)]\tLoss: 0.007749\nTrain Epoch: 1 [55000/60000 (92%)]\tLoss: 0.027474\nTrain Epoch: 1 [56000/60000 (93%)]\tLoss: 0.038733\nTrain Epoch: 1 [57000/60000 (95%)]\tLoss: 0.001992\nTrain Epoch: 1 [58000/60000 (97%)]\tLoss: 0.104928\nTrain Epoch: 1 [59000/60000 (98%)]\tLoss: 0.064666\n44.94s\n\nTest set: Average loss: 0.0714, Accuracy: 9783/10000 (98%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.000455\nTrain Epoch: 2 [1000/60000 (2%)]\tLoss: 0.084097\nTrain Epoch: 2 [2000/60000 (3%)]\tLoss: 0.012297\nTrain Epoch: 2 [3000/60000 (5%)]\tLoss: 0.024693\nTrain Epoch: 2 [4000/60000 (7%)]\tLoss: 0.008464\nTrain Epoch: 2 [5000/60000 (8%)]\tLoss: 0.269485\nTrain Epoch: 2 [6000/60000 (10%)]\tLoss: 0.046322\nTrain Epoch: 2 [7000/60000 (12%)]\tLoss: 0.013465\nTrain Epoch: 2 [8000/60000 (13%)]\tLoss: 0.007059\nTrain Epoch: 2 [9000/60000 (15%)]\tLoss: 0.004759\nTrain Epoch: 2 [10000/60000 (17%)]\tLoss: 0.279498\nTrain Epoch: 2 [11000/60000 (18%)]\tLoss: 0.007110\nTrain Epoch: 2 [12000/60000 (20%)]\tLoss: 0.014900\nTrain Epoch: 2 [13000/60000 (22%)]\tLoss: 0.003307\nTrain Epoch: 2 [14000/60000 (23%)]\tLoss: 0.028576\nTrain Epoch: 2 [15000/60000 (25%)]\tLoss: 0.257743\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 0.096126\nTrain Epoch: 2 [17000/60000 (28%)]\tLoss: 0.415127\nTrain Epoch: 2 [18000/60000 (30%)]\tLoss: 0.331429\nTrain Epoch: 2 [19000/60000 (32%)]\tLoss: 0.001268\nTrain Epoch: 2 [20000/60000 (33%)]\tLoss: 0.313928\nTrain Epoch: 2 [21000/60000 (35%)]\tLoss: 0.000963\nTrain Epoch: 2 [22000/60000 (37%)]\tLoss: 0.002407\nTrain Epoch: 2 [23000/60000 (38%)]\tLoss: 0.006263\nTrain Epoch: 2 [24000/60000 (40%)]\tLoss: 0.003526\nTrain Epoch: 2 [25000/60000 (42%)]\tLoss: 0.003706\nTrain Epoch: 2 [26000/60000 (43%)]\tLoss: 0.011320\nTrain Epoch: 2 [27000/60000 (45%)]\tLoss: 0.003709\nTrain Epoch: 2 [28000/60000 (47%)]\tLoss: 0.022201\nTrain Epoch: 2 [29000/60000 (48%)]\tLoss: 0.456491\nTrain Epoch: 2 [30000/60000 (50%)]\tLoss: 0.002703\nTrain Epoch: 2 [31000/60000 (52%)]\tLoss: 0.011188\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.012023\nTrain Epoch: 2 [33000/60000 (55%)]\tLoss: 0.010423\nTrain Epoch: 2 [34000/60000 (57%)]\tLoss: 0.015311\nTrain Epoch: 2 [35000/60000 (58%)]\tLoss: 0.609351\nTrain Epoch: 2 [36000/60000 (60%)]\tLoss: 0.100783\nTrain Epoch: 2 [37000/60000 (62%)]\tLoss: 0.342288\nTrain Epoch: 2 [38000/60000 (63%)]\tLoss: 0.003938\nTrain Epoch: 2 [39000/60000 (65%)]\tLoss: 0.012474\nTrain Epoch: 2 [40000/60000 (67%)]\tLoss: 0.007197\nTrain Epoch: 2 [41000/60000 (68%)]\tLoss: 0.005061\nTrain Epoch: 2 [42000/60000 (70%)]\tLoss: 0.001854\nTrain Epoch: 2 [43000/60000 (72%)]\tLoss: 0.146418\nTrain Epoch: 2 [44000/60000 (73%)]\tLoss: 0.024430\nTrain Epoch: 2 [45000/60000 (75%)]\tLoss: 0.011453\nTrain Epoch: 2 [46000/60000 (77%)]\tLoss: 0.001041\nTrain Epoch: 2 [47000/60000 (78%)]\tLoss: 0.000934\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.001358\nTrain Epoch: 2 [49000/60000 (82%)]\tLoss: 0.016600\nTrain Epoch: 2 [50000/60000 (83%)]\tLoss: 0.049472\nTrain Epoch: 2 [51000/60000 (85%)]\tLoss: 0.045855\nTrain Epoch: 2 [52000/60000 (87%)]\tLoss: 0.008307\nTrain Epoch: 2 [53000/60000 (88%)]\tLoss: 0.016881\nTrain Epoch: 2 [54000/60000 (90%)]\tLoss: 0.046644\nTrain Epoch: 2 [55000/60000 (92%)]\tLoss: 0.001319\nTrain Epoch: 2 [56000/60000 (93%)]\tLoss: 0.001004\nTrain Epoch: 2 [57000/60000 (95%)]\tLoss: 0.008055\nTrain Epoch: 2 [58000/60000 (97%)]\tLoss: 0.005830\nTrain Epoch: 2 [59000/60000 (98%)]\tLoss: 0.000303\n39.87s\n\nTest set: Average loss: 0.0545, Accuracy: 9820/10000 (98%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.005967\nTrain Epoch: 3 [1000/60000 (2%)]\tLoss: 0.043064\nTrain Epoch: 3 [2000/60000 (3%)]\tLoss: 0.004552\nTrain Epoch: 3 [3000/60000 (5%)]\tLoss: 0.001286\nTrain Epoch: 3 [4000/60000 (7%)]\tLoss: 0.005973\nTrain Epoch: 3 [5000/60000 (8%)]\tLoss: 0.005533\nTrain Epoch: 3 [6000/60000 (10%)]\tLoss: 0.124979\nTrain Epoch: 3 [7000/60000 (12%)]\tLoss: 0.116473\nTrain Epoch: 3 [8000/60000 (13%)]\tLoss: 0.000937\nTrain Epoch: 3 [9000/60000 (15%)]\tLoss: 0.208440\nTrain Epoch: 3 [10000/60000 (17%)]\tLoss: 0.002798\nTrain Epoch: 3 [11000/60000 (18%)]\tLoss: 0.001168\nTrain Epoch: 3 [12000/60000 (20%)]\tLoss: 0.008119\nTrain Epoch: 3 [13000/60000 (22%)]\tLoss: 0.005312\nTrain Epoch: 3 [14000/60000 (23%)]\tLoss: 0.046449\nTrain Epoch: 3 [15000/60000 (25%)]\tLoss: 0.104381\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.005463\nTrain Epoch: 3 [17000/60000 (28%)]\tLoss: 0.052283\nTrain Epoch: 3 [18000/60000 (30%)]\tLoss: 0.018752\nTrain Epoch: 3 [19000/60000 (32%)]\tLoss: 0.003160\nTrain Epoch: 3 [20000/60000 (33%)]\tLoss: 0.030559\nTrain Epoch: 3 [21000/60000 (35%)]\tLoss: 0.790999\nTrain Epoch: 3 [22000/60000 (37%)]\tLoss: 0.010877\nTrain Epoch: 3 [23000/60000 (38%)]\tLoss: 0.012252\nTrain Epoch: 3 [24000/60000 (40%)]\tLoss: 0.004893\nTrain Epoch: 3 [25000/60000 (42%)]\tLoss: 0.006248\nTrain Epoch: 3 [26000/60000 (43%)]\tLoss: 0.032639\nTrain Epoch: 3 [27000/60000 (45%)]\tLoss: 0.002867\nTrain Epoch: 3 [28000/60000 (47%)]\tLoss: 0.003844\nTrain Epoch: 3 [29000/60000 (48%)]\tLoss: 0.008813\nTrain Epoch: 3 [30000/60000 (50%)]\tLoss: 0.001755\nTrain Epoch: 3 [31000/60000 (52%)]\tLoss: 0.003208\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.103601\nTrain Epoch: 3 [33000/60000 (55%)]\tLoss: 0.015300\nTrain Epoch: 3 [34000/60000 (57%)]\tLoss: 0.000383\nTrain Epoch: 3 [35000/60000 (58%)]\tLoss: 0.001282\nTrain Epoch: 3 [36000/60000 (60%)]\tLoss: 0.000416\nTrain Epoch: 3 [37000/60000 (62%)]\tLoss: 0.001261\nTrain Epoch: 3 [38000/60000 (63%)]\tLoss: 0.321459\nTrain Epoch: 3 [39000/60000 (65%)]\tLoss: 0.009280\nTrain Epoch: 3 [40000/60000 (67%)]\tLoss: 0.000606\nTrain Epoch: 3 [41000/60000 (68%)]\tLoss: 0.031939\nTrain Epoch: 3 [42000/60000 (70%)]\tLoss: 0.014989\nTrain Epoch: 3 [43000/60000 (72%)]\tLoss: 0.000680\nTrain Epoch: 3 [44000/60000 (73%)]\tLoss: 0.459165\nTrain Epoch: 3 [45000/60000 (75%)]\tLoss: 0.002864\nTrain Epoch: 3 [46000/60000 (77%)]\tLoss: 0.025103\nTrain Epoch: 3 [47000/60000 (78%)]\tLoss: 0.000951\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 0.115377\nTrain Epoch: 3 [49000/60000 (82%)]\tLoss: 0.001406\nTrain Epoch: 3 [50000/60000 (83%)]\tLoss: 0.037118\nTrain Epoch: 3 [51000/60000 (85%)]\tLoss: 0.144353\nTrain Epoch: 3 [52000/60000 (87%)]\tLoss: 0.051342\nTrain Epoch: 3 [53000/60000 (88%)]\tLoss: 0.172235\nTrain Epoch: 3 [54000/60000 (90%)]\tLoss: 0.193469\nTrain Epoch: 3 [55000/60000 (92%)]\tLoss: 0.001991\nTrain Epoch: 3 [56000/60000 (93%)]\tLoss: 0.002786\nTrain Epoch: 3 [57000/60000 (95%)]\tLoss: 0.046262\nTrain Epoch: 3 [58000/60000 (97%)]\tLoss: 0.015513\nTrain Epoch: 3 [59000/60000 (98%)]\tLoss: 0.010270\n39.41s\n\nTest set: Average loss: 0.0474, Accuracy: 9844/10000 (98%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.063509\nTrain Epoch: 4 [1000/60000 (2%)]\tLoss: 0.001256\nTrain Epoch: 4 [2000/60000 (3%)]\tLoss: 0.135401\nTrain Epoch: 4 [3000/60000 (5%)]\tLoss: 0.005510\nTrain Epoch: 4 [4000/60000 (7%)]\tLoss: 0.006080\nTrain Epoch: 4 [5000/60000 (8%)]\tLoss: 0.021617\nTrain Epoch: 4 [6000/60000 (10%)]\tLoss: 0.005126\nTrain Epoch: 4 [7000/60000 (12%)]\tLoss: 0.000125\nTrain Epoch: 4 [8000/60000 (13%)]\tLoss: 0.013025\nTrain Epoch: 4 [9000/60000 (15%)]\tLoss: 0.000454\nTrain Epoch: 4 [10000/60000 (17%)]\tLoss: 0.020759\nTrain Epoch: 4 [11000/60000 (18%)]\tLoss: 0.000905\nTrain Epoch: 4 [12000/60000 (20%)]\tLoss: 0.044958\nTrain Epoch: 4 [13000/60000 (22%)]\tLoss: 0.000813\nTrain Epoch: 4 [14000/60000 (23%)]\tLoss: 0.069851\nTrain Epoch: 4 [15000/60000 (25%)]\tLoss: 0.003003\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 0.000055\nTrain Epoch: 4 [17000/60000 (28%)]\tLoss: 0.007029\nTrain Epoch: 4 [18000/60000 (30%)]\tLoss: 0.002494\nTrain Epoch: 4 [19000/60000 (32%)]\tLoss: 0.000979\nTrain Epoch: 4 [20000/60000 (33%)]\tLoss: 0.056651\nTrain Epoch: 4 [21000/60000 (35%)]\tLoss: 0.729531\nTrain Epoch: 4 [22000/60000 (37%)]\tLoss: 0.001472\nTrain Epoch: 4 [23000/60000 (38%)]\tLoss: 0.029722\nTrain Epoch: 4 [24000/60000 (40%)]\tLoss: 0.001204\nTrain Epoch: 4 [25000/60000 (42%)]\tLoss: 0.014069\nTrain Epoch: 4 [26000/60000 (43%)]\tLoss: 0.002233\nTrain Epoch: 4 [27000/60000 (45%)]\tLoss: 0.009170\nTrain Epoch: 4 [28000/60000 (47%)]\tLoss: 0.145281\nTrain Epoch: 4 [29000/60000 (48%)]\tLoss: 0.339723\nTrain Epoch: 4 [30000/60000 (50%)]\tLoss: 0.009413\nTrain Epoch: 4 [31000/60000 (52%)]\tLoss: 0.001509\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.091085\nTrain Epoch: 4 [33000/60000 (55%)]\tLoss: 0.023088\nTrain Epoch: 4 [34000/60000 (57%)]\tLoss: 0.000109\nTrain Epoch: 4 [35000/60000 (58%)]\tLoss: 0.078907\nTrain Epoch: 4 [36000/60000 (60%)]\tLoss: 0.033595\nTrain Epoch: 4 [37000/60000 (62%)]\tLoss: 0.008425\nTrain Epoch: 4 [38000/60000 (63%)]\tLoss: 0.001368\nTrain Epoch: 4 [39000/60000 (65%)]\tLoss: 0.003108\nTrain Epoch: 4 [40000/60000 (67%)]\tLoss: 0.005995\nTrain Epoch: 4 [41000/60000 (68%)]\tLoss: 0.206690\nTrain Epoch: 4 [42000/60000 (70%)]\tLoss: 0.000324\nTrain Epoch: 4 [43000/60000 (72%)]\tLoss: 0.012622\nTrain Epoch: 4 [44000/60000 (73%)]\tLoss: 0.000096\nTrain Epoch: 4 [45000/60000 (75%)]\tLoss: 0.001902\nTrain Epoch: 4 [46000/60000 (77%)]\tLoss: 0.000552\nTrain Epoch: 4 [47000/60000 (78%)]\tLoss: 0.045396\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 0.121565\nTrain Epoch: 4 [49000/60000 (82%)]\tLoss: 0.002579\nTrain Epoch: 4 [50000/60000 (83%)]\tLoss: 0.012141\nTrain Epoch: 4 [51000/60000 (85%)]\tLoss: 0.006725\nTrain Epoch: 4 [52000/60000 (87%)]\tLoss: 0.335810\nTrain Epoch: 4 [53000/60000 (88%)]\tLoss: 0.040576\nTrain Epoch: 4 [54000/60000 (90%)]\tLoss: 0.001648\nTrain Epoch: 4 [55000/60000 (92%)]\tLoss: 0.011369\nTrain Epoch: 4 [56000/60000 (93%)]\tLoss: 0.003494\nTrain Epoch: 4 [57000/60000 (95%)]\tLoss: 0.008021\nTrain Epoch: 4 [58000/60000 (97%)]\tLoss: 0.019015\nTrain Epoch: 4 [59000/60000 (98%)]\tLoss: 0.017617\n44.87s\n\nTest set: Average loss: 0.0410, Accuracy: 9867/10000 (99%)\n\n"
    }
   ],
   "source": [
    "train(5)  # train 5 epochs should get you to about 97% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save the model (model checkpointing)\n",
    "\n",
    "Now we have trained a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizer’s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTest set: Average loss: 2.3039, Accuracy: 1009/10000 (10%)\n\n"
    }
   ],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Testing -- you should get a pretty poor performance since the model hasn't learned anything yet.\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            # different from before: saving model checkpoints\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302032\nTrain Epoch: 0 [1000/60000 (2%)]\tLoss: 2.314921\nTrain Epoch: 0 [2000/60000 (3%)]\tLoss: 2.273562\nTrain Epoch: 0 [3000/60000 (5%)]\tLoss: 2.225657\nTrain Epoch: 0 [4000/60000 (7%)]\tLoss: 2.292648\nTrain Epoch: 0 [5000/60000 (8%)]\tLoss: 2.246100\nmodel saved to mnist-500.pth\nTrain Epoch: 0 [6000/60000 (10%)]\tLoss: 1.795047\nTrain Epoch: 0 [7000/60000 (12%)]\tLoss: 1.571782\nTrain Epoch: 0 [8000/60000 (13%)]\tLoss: 2.007380\nTrain Epoch: 0 [9000/60000 (15%)]\tLoss: 1.289588\nTrain Epoch: 0 [10000/60000 (17%)]\tLoss: 1.059608\nmodel saved to mnist-1000.pth\nTrain Epoch: 0 [11000/60000 (18%)]\tLoss: 0.819897\nTrain Epoch: 0 [12000/60000 (20%)]\tLoss: 0.786397\nTrain Epoch: 0 [13000/60000 (22%)]\tLoss: 0.750669\nTrain Epoch: 0 [14000/60000 (23%)]\tLoss: 1.526305\nTrain Epoch: 0 [15000/60000 (25%)]\tLoss: 0.716481\nmodel saved to mnist-1500.pth\nTrain Epoch: 0 [16000/60000 (27%)]\tLoss: 0.902609\nTrain Epoch: 0 [17000/60000 (28%)]\tLoss: 0.808425\nTrain Epoch: 0 [18000/60000 (30%)]\tLoss: 0.463886\nTrain Epoch: 0 [19000/60000 (32%)]\tLoss: 0.521275\nTrain Epoch: 0 [20000/60000 (33%)]\tLoss: 0.372958\nmodel saved to mnist-2000.pth\nTrain Epoch: 0 [21000/60000 (35%)]\tLoss: 0.606425\nTrain Epoch: 0 [22000/60000 (37%)]\tLoss: 0.484394\nTrain Epoch: 0 [23000/60000 (38%)]\tLoss: 0.859020\nTrain Epoch: 0 [24000/60000 (40%)]\tLoss: 0.284099\nTrain Epoch: 0 [25000/60000 (42%)]\tLoss: 0.800880\nmodel saved to mnist-2500.pth\nTrain Epoch: 0 [26000/60000 (43%)]\tLoss: 0.688332\nTrain Epoch: 0 [27000/60000 (45%)]\tLoss: 0.514345\nTrain Epoch: 0 [28000/60000 (47%)]\tLoss: 0.221901\nTrain Epoch: 0 [29000/60000 (48%)]\tLoss: 0.254881\nTrain Epoch: 0 [30000/60000 (50%)]\tLoss: 0.092757\nmodel saved to mnist-3000.pth\nTrain Epoch: 0 [31000/60000 (52%)]\tLoss: 0.342654\nTrain Epoch: 0 [32000/60000 (53%)]\tLoss: 0.199953\nTrain Epoch: 0 [33000/60000 (55%)]\tLoss: 0.218084\nTrain Epoch: 0 [34000/60000 (57%)]\tLoss: 0.848303\nTrain Epoch: 0 [35000/60000 (58%)]\tLoss: 0.515951\nmodel saved to mnist-3500.pth\nTrain Epoch: 0 [36000/60000 (60%)]\tLoss: 0.531606\nTrain Epoch: 0 [37000/60000 (62%)]\tLoss: 1.030056\nTrain Epoch: 0 [38000/60000 (63%)]\tLoss: 0.834452\nTrain Epoch: 0 [39000/60000 (65%)]\tLoss: 0.457013\nTrain Epoch: 0 [40000/60000 (67%)]\tLoss: 0.277274\nmodel saved to mnist-4000.pth\nTrain Epoch: 0 [41000/60000 (68%)]\tLoss: 0.471687\nTrain Epoch: 0 [42000/60000 (70%)]\tLoss: 1.190963\nTrain Epoch: 0 [43000/60000 (72%)]\tLoss: 0.284734\nTrain Epoch: 0 [44000/60000 (73%)]\tLoss: 0.459381\nTrain Epoch: 0 [45000/60000 (75%)]\tLoss: 0.630984\nmodel saved to mnist-4500.pth\nTrain Epoch: 0 [46000/60000 (77%)]\tLoss: 0.443459\nTrain Epoch: 0 [47000/60000 (78%)]\tLoss: 0.708019\nTrain Epoch: 0 [48000/60000 (80%)]\tLoss: 0.717811\nTrain Epoch: 0 [49000/60000 (82%)]\tLoss: 0.386214\nTrain Epoch: 0 [50000/60000 (83%)]\tLoss: 1.223190\nmodel saved to mnist-5000.pth\nTrain Epoch: 0 [51000/60000 (85%)]\tLoss: 2.112970\nTrain Epoch: 0 [52000/60000 (87%)]\tLoss: 0.165917\nTrain Epoch: 0 [53000/60000 (88%)]\tLoss: 0.457717\nTrain Epoch: 0 [54000/60000 (90%)]\tLoss: 0.310370\nTrain Epoch: 0 [55000/60000 (92%)]\tLoss: 0.450865\nmodel saved to mnist-5500.pth\nTrain Epoch: 0 [56000/60000 (93%)]\tLoss: 0.418906\nTrain Epoch: 0 [57000/60000 (95%)]\tLoss: 0.123961\nTrain Epoch: 0 [58000/60000 (97%)]\tLoss: 0.252968\nTrain Epoch: 0 [59000/60000 (98%)]\tLoss: 0.184164\n\nTest set: Average loss: 0.1566, Accuracy: 9538/10000 (95%)\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 0.182190\nmodel saved to mnist-6000.pth\nTrain Epoch: 1 [1000/60000 (2%)]\tLoss: 0.017129\nTrain Epoch: 1 [2000/60000 (3%)]\tLoss: 0.006923\nTrain Epoch: 1 [3000/60000 (5%)]\tLoss: 0.026587\nTrain Epoch: 1 [4000/60000 (7%)]\tLoss: 0.029301\nTrain Epoch: 1 [5000/60000 (8%)]\tLoss: 0.035330\nmodel saved to mnist-6500.pth\nTrain Epoch: 1 [6000/60000 (10%)]\tLoss: 0.464135\nTrain Epoch: 1 [7000/60000 (12%)]\tLoss: 0.803071\nTrain Epoch: 1 [8000/60000 (13%)]\tLoss: 0.179449\nTrain Epoch: 1 [9000/60000 (15%)]\tLoss: 0.004185\nTrain Epoch: 1 [10000/60000 (17%)]\tLoss: 0.057299\nmodel saved to mnist-7000.pth\nTrain Epoch: 1 [11000/60000 (18%)]\tLoss: 0.162717\nTrain Epoch: 1 [12000/60000 (20%)]\tLoss: 0.019194\nTrain Epoch: 1 [13000/60000 (22%)]\tLoss: 0.018491\nTrain Epoch: 1 [14000/60000 (23%)]\tLoss: 0.136356\nTrain Epoch: 1 [15000/60000 (25%)]\tLoss: 0.099211\nmodel saved to mnist-7500.pth\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 0.032071\nTrain Epoch: 1 [17000/60000 (28%)]\tLoss: 0.290497\nTrain Epoch: 1 [18000/60000 (30%)]\tLoss: 0.144578\nTrain Epoch: 1 [19000/60000 (32%)]\tLoss: 0.137366\nTrain Epoch: 1 [20000/60000 (33%)]\tLoss: 0.049482\nmodel saved to mnist-8000.pth\nTrain Epoch: 1 [21000/60000 (35%)]\tLoss: 0.005827\nTrain Epoch: 1 [22000/60000 (37%)]\tLoss: 0.008344\nTrain Epoch: 1 [23000/60000 (38%)]\tLoss: 0.096974\nTrain Epoch: 1 [24000/60000 (40%)]\tLoss: 0.004161\nTrain Epoch: 1 [25000/60000 (42%)]\tLoss: 0.088044\nmodel saved to mnist-8500.pth\nTrain Epoch: 1 [26000/60000 (43%)]\tLoss: 0.032639\nTrain Epoch: 1 [27000/60000 (45%)]\tLoss: 0.291905\nTrain Epoch: 1 [28000/60000 (47%)]\tLoss: 0.036982\nTrain Epoch: 1 [29000/60000 (48%)]\tLoss: 0.093297\nTrain Epoch: 1 [30000/60000 (50%)]\tLoss: 0.037417\nmodel saved to mnist-9000.pth\nTrain Epoch: 1 [31000/60000 (52%)]\tLoss: 0.185841\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.006034\nTrain Epoch: 1 [33000/60000 (55%)]\tLoss: 0.010775\nTrain Epoch: 1 [34000/60000 (57%)]\tLoss: 0.040652\nTrain Epoch: 1 [35000/60000 (58%)]\tLoss: 0.015257\nmodel saved to mnist-9500.pth\nTrain Epoch: 1 [36000/60000 (60%)]\tLoss: 0.368952\nTrain Epoch: 1 [37000/60000 (62%)]\tLoss: 0.004921\nTrain Epoch: 1 [38000/60000 (63%)]\tLoss: 0.420935\nTrain Epoch: 1 [39000/60000 (65%)]\tLoss: 0.320226\nTrain Epoch: 1 [40000/60000 (67%)]\tLoss: 0.034222\nmodel saved to mnist-10000.pth\nTrain Epoch: 1 [41000/60000 (68%)]\tLoss: 0.002647\nTrain Epoch: 1 [42000/60000 (70%)]\tLoss: 0.019805\nTrain Epoch: 1 [43000/60000 (72%)]\tLoss: 0.034294\nTrain Epoch: 1 [44000/60000 (73%)]\tLoss: 0.045701\nTrain Epoch: 1 [45000/60000 (75%)]\tLoss: 0.276372\nmodel saved to mnist-10500.pth\nTrain Epoch: 1 [46000/60000 (77%)]\tLoss: 0.003681\nTrain Epoch: 1 [47000/60000 (78%)]\tLoss: 0.100700\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.002542\nTrain Epoch: 1 [49000/60000 (82%)]\tLoss: 0.012232\nTrain Epoch: 1 [50000/60000 (83%)]\tLoss: 0.108218\nmodel saved to mnist-11000.pth\nTrain Epoch: 1 [51000/60000 (85%)]\tLoss: 0.005547\nTrain Epoch: 1 [52000/60000 (87%)]\tLoss: 0.083594\nTrain Epoch: 1 [53000/60000 (88%)]\tLoss: 0.025841\nTrain Epoch: 1 [54000/60000 (90%)]\tLoss: 0.001279\nTrain Epoch: 1 [55000/60000 (92%)]\tLoss: 0.006261\nmodel saved to mnist-11500.pth\nTrain Epoch: 1 [56000/60000 (93%)]\tLoss: 0.139309\nTrain Epoch: 1 [57000/60000 (95%)]\tLoss: 0.402928\nTrain Epoch: 1 [58000/60000 (97%)]\tLoss: 0.122927\nTrain Epoch: 1 [59000/60000 (98%)]\tLoss: 0.006152\n\nTest set: Average loss: 0.0734, Accuracy: 9747/10000 (97%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.004637\nmodel saved to mnist-12000.pth\nTrain Epoch: 2 [1000/60000 (2%)]\tLoss: 0.073305\nTrain Epoch: 2 [2000/60000 (3%)]\tLoss: 0.007685\nTrain Epoch: 2 [3000/60000 (5%)]\tLoss: 0.015871\nTrain Epoch: 2 [4000/60000 (7%)]\tLoss: 0.066294\nTrain Epoch: 2 [5000/60000 (8%)]\tLoss: 0.019536\nmodel saved to mnist-12500.pth\nTrain Epoch: 2 [6000/60000 (10%)]\tLoss: 0.216978\nTrain Epoch: 2 [7000/60000 (12%)]\tLoss: 0.043384\nTrain Epoch: 2 [8000/60000 (13%)]\tLoss: 0.008167\nTrain Epoch: 2 [9000/60000 (15%)]\tLoss: 0.008050\nTrain Epoch: 2 [10000/60000 (17%)]\tLoss: 0.105462\nmodel saved to mnist-13000.pth\nTrain Epoch: 2 [11000/60000 (18%)]\tLoss: 0.012662\nTrain Epoch: 2 [12000/60000 (20%)]\tLoss: 0.237528\nTrain Epoch: 2 [13000/60000 (22%)]\tLoss: 0.001703\nTrain Epoch: 2 [14000/60000 (23%)]\tLoss: 0.179885\nTrain Epoch: 2 [15000/60000 (25%)]\tLoss: 0.096463\nmodel saved to mnist-13500.pth\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 0.008045\nTrain Epoch: 2 [17000/60000 (28%)]\tLoss: 0.035121\nTrain Epoch: 2 [18000/60000 (30%)]\tLoss: 0.065407\nTrain Epoch: 2 [19000/60000 (32%)]\tLoss: 0.025023\nTrain Epoch: 2 [20000/60000 (33%)]\tLoss: 0.002649\nmodel saved to mnist-14000.pth\nTrain Epoch: 2 [21000/60000 (35%)]\tLoss: 0.153671\nTrain Epoch: 2 [22000/60000 (37%)]\tLoss: 0.030185\nTrain Epoch: 2 [23000/60000 (38%)]\tLoss: 0.050740\nTrain Epoch: 2 [24000/60000 (40%)]\tLoss: 0.113006\nTrain Epoch: 2 [25000/60000 (42%)]\tLoss: 0.011585\nmodel saved to mnist-14500.pth\nTrain Epoch: 2 [26000/60000 (43%)]\tLoss: 0.000556\nTrain Epoch: 2 [27000/60000 (45%)]\tLoss: 0.074980\nTrain Epoch: 2 [28000/60000 (47%)]\tLoss: 0.032869\nTrain Epoch: 2 [29000/60000 (48%)]\tLoss: 0.004931\nTrain Epoch: 2 [30000/60000 (50%)]\tLoss: 0.021554\nmodel saved to mnist-15000.pth\nTrain Epoch: 2 [31000/60000 (52%)]\tLoss: 0.009876\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.041942\nTrain Epoch: 2 [33000/60000 (55%)]\tLoss: 0.021434\nTrain Epoch: 2 [34000/60000 (57%)]\tLoss: 0.002872\nTrain Epoch: 2 [35000/60000 (58%)]\tLoss: 0.060110\nmodel saved to mnist-15500.pth\nTrain Epoch: 2 [36000/60000 (60%)]\tLoss: 0.107723\nTrain Epoch: 2 [37000/60000 (62%)]\tLoss: 0.312572\nTrain Epoch: 2 [38000/60000 (63%)]\tLoss: 0.006568\nTrain Epoch: 2 [39000/60000 (65%)]\tLoss: 0.005181\nTrain Epoch: 2 [40000/60000 (67%)]\tLoss: 0.160132\nmodel saved to mnist-16000.pth\nTrain Epoch: 2 [41000/60000 (68%)]\tLoss: 0.000210\nTrain Epoch: 2 [42000/60000 (70%)]\tLoss: 0.001559\nTrain Epoch: 2 [43000/60000 (72%)]\tLoss: 0.001248\nTrain Epoch: 2 [44000/60000 (73%)]\tLoss: 0.012201\nTrain Epoch: 2 [45000/60000 (75%)]\tLoss: 0.143830\nmodel saved to mnist-16500.pth\nTrain Epoch: 2 [46000/60000 (77%)]\tLoss: 0.064825\nTrain Epoch: 2 [47000/60000 (78%)]\tLoss: 0.021841\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.007148\nTrain Epoch: 2 [49000/60000 (82%)]\tLoss: 0.002322\nTrain Epoch: 2 [50000/60000 (83%)]\tLoss: 0.594929\nmodel saved to mnist-17000.pth\nTrain Epoch: 2 [51000/60000 (85%)]\tLoss: 0.035592\nTrain Epoch: 2 [52000/60000 (87%)]\tLoss: 0.029375\nTrain Epoch: 2 [53000/60000 (88%)]\tLoss: 0.015585\nTrain Epoch: 2 [54000/60000 (90%)]\tLoss: 0.003463\nTrain Epoch: 2 [55000/60000 (92%)]\tLoss: 0.012185\nmodel saved to mnist-17500.pth\nTrain Epoch: 2 [56000/60000 (93%)]\tLoss: 0.045436\nTrain Epoch: 2 [57000/60000 (95%)]\tLoss: 0.146326\nTrain Epoch: 2 [58000/60000 (97%)]\tLoss: 0.000520\nTrain Epoch: 2 [59000/60000 (98%)]\tLoss: 0.040831\n\nTest set: Average loss: 0.0553, Accuracy: 9820/10000 (98%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.014562\nmodel saved to mnist-18000.pth\nTrain Epoch: 3 [1000/60000 (2%)]\tLoss: 0.000074\nTrain Epoch: 3 [2000/60000 (3%)]\tLoss: 0.001350\nTrain Epoch: 3 [3000/60000 (5%)]\tLoss: 0.021549\nTrain Epoch: 3 [4000/60000 (7%)]\tLoss: 0.000362\nTrain Epoch: 3 [5000/60000 (8%)]\tLoss: 0.016164\nmodel saved to mnist-18500.pth\nTrain Epoch: 3 [6000/60000 (10%)]\tLoss: 0.158564\nTrain Epoch: 3 [7000/60000 (12%)]\tLoss: 0.002882\nTrain Epoch: 3 [8000/60000 (13%)]\tLoss: 0.174811\nTrain Epoch: 3 [9000/60000 (15%)]\tLoss: 0.001559\nTrain Epoch: 3 [10000/60000 (17%)]\tLoss: 0.001595\nmodel saved to mnist-19000.pth\nTrain Epoch: 3 [11000/60000 (18%)]\tLoss: 0.000756\nTrain Epoch: 3 [12000/60000 (20%)]\tLoss: 0.059591\nTrain Epoch: 3 [13000/60000 (22%)]\tLoss: 0.003835\nTrain Epoch: 3 [14000/60000 (23%)]\tLoss: 0.075243\nTrain Epoch: 3 [15000/60000 (25%)]\tLoss: 0.048781\nmodel saved to mnist-19500.pth\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.016739\nTrain Epoch: 3 [17000/60000 (28%)]\tLoss: 0.016181\nTrain Epoch: 3 [18000/60000 (30%)]\tLoss: 0.476711\nTrain Epoch: 3 [19000/60000 (32%)]\tLoss: 0.001807\nTrain Epoch: 3 [20000/60000 (33%)]\tLoss: 0.008600\nmodel saved to mnist-20000.pth\nTrain Epoch: 3 [21000/60000 (35%)]\tLoss: 0.127122\nTrain Epoch: 3 [22000/60000 (37%)]\tLoss: 0.001605\nTrain Epoch: 3 [23000/60000 (38%)]\tLoss: 0.004536\nTrain Epoch: 3 [24000/60000 (40%)]\tLoss: 0.001584\nTrain Epoch: 3 [25000/60000 (42%)]\tLoss: 0.149735\nmodel saved to mnist-20500.pth\nTrain Epoch: 3 [26000/60000 (43%)]\tLoss: 0.015492\nTrain Epoch: 3 [27000/60000 (45%)]\tLoss: 0.162808\nTrain Epoch: 3 [28000/60000 (47%)]\tLoss: 0.000903\nTrain Epoch: 3 [29000/60000 (48%)]\tLoss: 0.103152\nTrain Epoch: 3 [30000/60000 (50%)]\tLoss: 0.001039\nmodel saved to mnist-21000.pth\nTrain Epoch: 3 [31000/60000 (52%)]\tLoss: 0.006594\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.037672\nTrain Epoch: 3 [33000/60000 (55%)]\tLoss: 0.007388\nTrain Epoch: 3 [34000/60000 (57%)]\tLoss: 0.011204\nTrain Epoch: 3 [35000/60000 (58%)]\tLoss: 0.010780\nmodel saved to mnist-21500.pth\nTrain Epoch: 3 [36000/60000 (60%)]\tLoss: 0.000725\nTrain Epoch: 3 [37000/60000 (62%)]\tLoss: 0.008170\nTrain Epoch: 3 [38000/60000 (63%)]\tLoss: 0.096323\nTrain Epoch: 3 [39000/60000 (65%)]\tLoss: 0.003604\nTrain Epoch: 3 [40000/60000 (67%)]\tLoss: 0.041627\nmodel saved to mnist-22000.pth\nTrain Epoch: 3 [41000/60000 (68%)]\tLoss: 0.001630\nTrain Epoch: 3 [42000/60000 (70%)]\tLoss: 0.007742\nTrain Epoch: 3 [43000/60000 (72%)]\tLoss: 0.043443\nTrain Epoch: 3 [44000/60000 (73%)]\tLoss: 0.030918\nTrain Epoch: 3 [45000/60000 (75%)]\tLoss: 0.014447\nmodel saved to mnist-22500.pth\nTrain Epoch: 3 [46000/60000 (77%)]\tLoss: 0.039060\nTrain Epoch: 3 [47000/60000 (78%)]\tLoss: 0.010790\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 0.429678\nTrain Epoch: 3 [49000/60000 (82%)]\tLoss: 0.000927\nTrain Epoch: 3 [50000/60000 (83%)]\tLoss: 0.000446\nmodel saved to mnist-23000.pth\nTrain Epoch: 3 [51000/60000 (85%)]\tLoss: 0.207940\nTrain Epoch: 3 [52000/60000 (87%)]\tLoss: 0.020496\nTrain Epoch: 3 [53000/60000 (88%)]\tLoss: 0.005849\nTrain Epoch: 3 [54000/60000 (90%)]\tLoss: 0.015153\nTrain Epoch: 3 [55000/60000 (92%)]\tLoss: 0.467390\nmodel saved to mnist-23500.pth\nTrain Epoch: 3 [56000/60000 (93%)]\tLoss: 0.000834\nTrain Epoch: 3 [57000/60000 (95%)]\tLoss: 0.001072\nTrain Epoch: 3 [58000/60000 (97%)]\tLoss: 0.013851\nTrain Epoch: 3 [59000/60000 (98%)]\tLoss: 0.104394\n\nTest set: Average loss: 0.0445, Accuracy: 9852/10000 (99%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.000331\nmodel saved to mnist-24000.pth\nTrain Epoch: 4 [1000/60000 (2%)]\tLoss: 0.358402\nTrain Epoch: 4 [2000/60000 (3%)]\tLoss: 0.227925\nTrain Epoch: 4 [3000/60000 (5%)]\tLoss: 0.002030\nTrain Epoch: 4 [4000/60000 (7%)]\tLoss: 0.000467\nTrain Epoch: 4 [5000/60000 (8%)]\tLoss: 0.110987\nmodel saved to mnist-24500.pth\nTrain Epoch: 4 [6000/60000 (10%)]\tLoss: 0.006749\nTrain Epoch: 4 [7000/60000 (12%)]\tLoss: 0.002107\nTrain Epoch: 4 [8000/60000 (13%)]\tLoss: 0.044607\nTrain Epoch: 4 [9000/60000 (15%)]\tLoss: 0.008535\nTrain Epoch: 4 [10000/60000 (17%)]\tLoss: 0.000382\nmodel saved to mnist-25000.pth\nTrain Epoch: 4 [11000/60000 (18%)]\tLoss: 0.001671\nTrain Epoch: 4 [12000/60000 (20%)]\tLoss: 0.000923\nTrain Epoch: 4 [13000/60000 (22%)]\tLoss: 0.001471\nTrain Epoch: 4 [14000/60000 (23%)]\tLoss: 0.001609\nTrain Epoch: 4 [15000/60000 (25%)]\tLoss: 0.001946\nmodel saved to mnist-25500.pth\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 0.047689\nTrain Epoch: 4 [17000/60000 (28%)]\tLoss: 0.003349\nTrain Epoch: 4 [18000/60000 (30%)]\tLoss: 0.001836\nTrain Epoch: 4 [19000/60000 (32%)]\tLoss: 0.013219\nTrain Epoch: 4 [20000/60000 (33%)]\tLoss: 0.014630\nmodel saved to mnist-26000.pth\nTrain Epoch: 4 [21000/60000 (35%)]\tLoss: 0.001067\nTrain Epoch: 4 [22000/60000 (37%)]\tLoss: 0.001825\nTrain Epoch: 4 [23000/60000 (38%)]\tLoss: 0.002020\nTrain Epoch: 4 [24000/60000 (40%)]\tLoss: 0.001532\nTrain Epoch: 4 [25000/60000 (42%)]\tLoss: 0.000586\nmodel saved to mnist-26500.pth\nTrain Epoch: 4 [26000/60000 (43%)]\tLoss: 0.002175\nTrain Epoch: 4 [27000/60000 (45%)]\tLoss: 0.000651\nTrain Epoch: 4 [28000/60000 (47%)]\tLoss: 0.001690\nTrain Epoch: 4 [29000/60000 (48%)]\tLoss: 0.192331\nTrain Epoch: 4 [30000/60000 (50%)]\tLoss: 0.015371\nmodel saved to mnist-27000.pth\nTrain Epoch: 4 [31000/60000 (52%)]\tLoss: 0.008470\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.018249\nTrain Epoch: 4 [33000/60000 (55%)]\tLoss: 0.473540\nTrain Epoch: 4 [34000/60000 (57%)]\tLoss: 0.367179\nTrain Epoch: 4 [35000/60000 (58%)]\tLoss: 0.004129\nmodel saved to mnist-27500.pth\nTrain Epoch: 4 [36000/60000 (60%)]\tLoss: 0.038571\nTrain Epoch: 4 [37000/60000 (62%)]\tLoss: 0.020932\nTrain Epoch: 4 [38000/60000 (63%)]\tLoss: 0.263781\nTrain Epoch: 4 [39000/60000 (65%)]\tLoss: 0.004201\nTrain Epoch: 4 [40000/60000 (67%)]\tLoss: 0.017869\nmodel saved to mnist-28000.pth\nTrain Epoch: 4 [41000/60000 (68%)]\tLoss: 0.017340\nTrain Epoch: 4 [42000/60000 (70%)]\tLoss: 0.005275\nTrain Epoch: 4 [43000/60000 (72%)]\tLoss: 0.011619\nTrain Epoch: 4 [44000/60000 (73%)]\tLoss: 0.001185\nTrain Epoch: 4 [45000/60000 (75%)]\tLoss: 0.028000\nmodel saved to mnist-28500.pth\nTrain Epoch: 4 [46000/60000 (77%)]\tLoss: 0.007666\nTrain Epoch: 4 [47000/60000 (78%)]\tLoss: 0.164397\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 0.004012\nTrain Epoch: 4 [49000/60000 (82%)]\tLoss: 0.001640\nTrain Epoch: 4 [50000/60000 (83%)]\tLoss: 0.002470\nmodel saved to mnist-29000.pth\nTrain Epoch: 4 [51000/60000 (85%)]\tLoss: 0.005127\nTrain Epoch: 4 [52000/60000 (87%)]\tLoss: 0.000883\nTrain Epoch: 4 [53000/60000 (88%)]\tLoss: 0.006526\nTrain Epoch: 4 [54000/60000 (90%)]\tLoss: 0.003862\nTrain Epoch: 4 [55000/60000 (92%)]\tLoss: 0.003056\nmodel saved to mnist-29500.pth\nTrain Epoch: 4 [56000/60000 (93%)]\tLoss: 0.058271\nTrain Epoch: 4 [57000/60000 (95%)]\tLoss: 0.001270\nTrain Epoch: 4 [58000/60000 (97%)]\tLoss: 0.000773\nTrain Epoch: 4 [59000/60000 (98%)]\tLoss: 0.168164\n\nTest set: Average loss: 0.0408, Accuracy: 9880/10000 (99%)\n\nmodel saved to mnist-30000.pth\n"
    }
   ],
   "source": [
    "train_save(5, save_interval=500, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist-4690.pth'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4ac65a011571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load from the final checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist-4690.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# should give you the final model accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-479c5ccd3026>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[1;34m(checkpoint_path, model, optimizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist-4690.pth'"
     ]
    }
   ],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune a model\n",
    "\n",
    "Sometimes you want to fine-tune a pretrained model instead of training a model from scratch. For example, if you want to train a model on a new dataset that contains natural images. To achieve the best performance, you can start with a model that's fully trained on ImageNet and fine-tune the model.\n",
    "\n",
    "Finetuning a model in PyTorch is super easy! First, let's find out what we saved in a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
    }
   ],
   "source": [
    "# What's in a state dict?\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the fc layers\n",
    "\n",
    "Now say we want to load the conv layers from the checkpoint and train the fc layers. We can simply load a subset of the state dict with the selected names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist-4690.pth'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0813b74077df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist-4690.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstates_to_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# only load the conv layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist-4690.pth'"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        # only load the conv layers\n",
    "        states_to_load[name] = param\n",
    "print(\"Number of parameter variables to load:\", len(states_to_load))\n",
    "\n",
    "# Construct a new state_dict in which the layers we want\n",
    "# to import from the checkpoint is updated with the parameters\n",
    "# from the checkpoint\n",
    "model = Net().to(device)\n",
    "model_state = model.state_dict()\n",
    "print(\"Number of parameter variables in the model:\", len(model_state))\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "model.load_state_dict(model_state)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTest set: Average loss: 2.3084, Accuracy: 969/10000 (10%)\n\nTrain Epoch: 0 [0/60000 (0%)]\tLoss: 2.287031\nTrain Epoch: 0 [1000/60000 (2%)]\tLoss: 2.315804\nTrain Epoch: 0 [2000/60000 (3%)]\tLoss: 2.310418\nTrain Epoch: 0 [3000/60000 (5%)]\tLoss: 2.331227\nTrain Epoch: 0 [4000/60000 (7%)]\tLoss: 2.293371\nTrain Epoch: 0 [5000/60000 (8%)]\tLoss: 2.266325\nTrain Epoch: 0 [6000/60000 (10%)]\tLoss: 2.221242\nTrain Epoch: 0 [7000/60000 (12%)]\tLoss: 2.149525\nTrain Epoch: 0 [8000/60000 (13%)]\tLoss: 1.980411\nTrain Epoch: 0 [9000/60000 (15%)]\tLoss: 1.529460\nTrain Epoch: 0 [10000/60000 (17%)]\tLoss: 1.698615\nTrain Epoch: 0 [11000/60000 (18%)]\tLoss: 1.133192\nTrain Epoch: 0 [12000/60000 (20%)]\tLoss: 1.499243\nTrain Epoch: 0 [13000/60000 (22%)]\tLoss: 1.252452\nTrain Epoch: 0 [14000/60000 (23%)]\tLoss: 0.996743\nTrain Epoch: 0 [15000/60000 (25%)]\tLoss: 0.790674\nTrain Epoch: 0 [16000/60000 (27%)]\tLoss: 1.478256\nTrain Epoch: 0 [17000/60000 (28%)]\tLoss: 0.565070\nTrain Epoch: 0 [18000/60000 (30%)]\tLoss: 0.383587\nTrain Epoch: 0 [19000/60000 (32%)]\tLoss: 0.529413\nTrain Epoch: 0 [20000/60000 (33%)]\tLoss: 0.885189\nTrain Epoch: 0 [21000/60000 (35%)]\tLoss: 0.639016\nTrain Epoch: 0 [22000/60000 (37%)]\tLoss: 0.555999\nTrain Epoch: 0 [23000/60000 (38%)]\tLoss: 0.181283\nTrain Epoch: 0 [24000/60000 (40%)]\tLoss: 0.914268\nTrain Epoch: 0 [25000/60000 (42%)]\tLoss: 0.389463\nTrain Epoch: 0 [26000/60000 (43%)]\tLoss: 1.046293\nTrain Epoch: 0 [27000/60000 (45%)]\tLoss: 0.475751\nTrain Epoch: 0 [28000/60000 (47%)]\tLoss: 0.268879\nTrain Epoch: 0 [29000/60000 (48%)]\tLoss: 1.350761\nTrain Epoch: 0 [30000/60000 (50%)]\tLoss: 1.080096\nTrain Epoch: 0 [31000/60000 (52%)]\tLoss: 0.303729\nTrain Epoch: 0 [32000/60000 (53%)]\tLoss: 0.470809\nTrain Epoch: 0 [33000/60000 (55%)]\tLoss: 1.159548\nTrain Epoch: 0 [34000/60000 (57%)]\tLoss: 0.204159\nTrain Epoch: 0 [35000/60000 (58%)]\tLoss: 0.939213\nTrain Epoch: 0 [36000/60000 (60%)]\tLoss: 0.461244\nTrain Epoch: 0 [37000/60000 (62%)]\tLoss: 0.193029\nTrain Epoch: 0 [38000/60000 (63%)]\tLoss: 0.341439\nTrain Epoch: 0 [39000/60000 (65%)]\tLoss: 0.365939\nTrain Epoch: 0 [40000/60000 (67%)]\tLoss: 0.498301\nTrain Epoch: 0 [41000/60000 (68%)]\tLoss: 0.235601\nTrain Epoch: 0 [42000/60000 (70%)]\tLoss: 0.467938\nTrain Epoch: 0 [43000/60000 (72%)]\tLoss: 0.289641\nTrain Epoch: 0 [44000/60000 (73%)]\tLoss: 0.461950\nTrain Epoch: 0 [45000/60000 (75%)]\tLoss: 0.437820\nTrain Epoch: 0 [46000/60000 (77%)]\tLoss: 0.453923\nTrain Epoch: 0 [47000/60000 (78%)]\tLoss: 1.554860\nTrain Epoch: 0 [48000/60000 (80%)]\tLoss: 0.416956\nTrain Epoch: 0 [49000/60000 (82%)]\tLoss: 0.705627\nTrain Epoch: 0 [50000/60000 (83%)]\tLoss: 0.883647\nTrain Epoch: 0 [51000/60000 (85%)]\tLoss: 0.679369\nTrain Epoch: 0 [52000/60000 (87%)]\tLoss: 0.503434\nTrain Epoch: 0 [53000/60000 (88%)]\tLoss: 0.824324\nTrain Epoch: 0 [54000/60000 (90%)]\tLoss: 0.344929\nTrain Epoch: 0 [55000/60000 (92%)]\tLoss: 0.660805\nTrain Epoch: 0 [56000/60000 (93%)]\tLoss: 0.286733\nTrain Epoch: 0 [57000/60000 (95%)]\tLoss: 0.266045\nTrain Epoch: 0 [58000/60000 (97%)]\tLoss: 0.551985\nTrain Epoch: 0 [59000/60000 (98%)]\tLoss: 0.818510\n40.78s\n\nTest set: Average loss: 0.1793, Accuracy: 9456/10000 (95%)\n\n"
    }
   ],
   "source": [
    "test() # without fine-tuning.\n",
    "\n",
    "train(1)  # training 1 epoch will get you to 93%!\n",
    "# As a comparison, training from scratch for 1 epoch gets about ~80% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pretrained weights in a different model\n",
    "\n",
    "We can even use the pretrained conv layers in a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        # same conv layers\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # fewer FC layers\n",
    "        self.fc1 = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = SmallNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist-4690.pth'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9ae4d4746cac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist-4690.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstates_to_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstates_to_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\App\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist-4690.pth'"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        states_to_load[name] = param\n",
    "\n",
    "# Construct a new state dict in which the layers we want\n",
    "# to import from the checkpoint is update with the parameters\n",
    "# from the checkpoint\n",
    "model_state = model.state_dict()\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "test()\n",
    "\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.256372\nTrain Epoch: 0 [1000/60000 (2%)]\tLoss: 2.287730\nTrain Epoch: 0 [2000/60000 (3%)]\tLoss: 1.977647\nTrain Epoch: 0 [3000/60000 (5%)]\tLoss: 1.418641\nTrain Epoch: 0 [4000/60000 (7%)]\tLoss: 1.443853\nTrain Epoch: 0 [5000/60000 (8%)]\tLoss: 1.252747\nTrain Epoch: 0 [6000/60000 (10%)]\tLoss: 0.471200\nTrain Epoch: 0 [7000/60000 (12%)]\tLoss: 0.310096\nTrain Epoch: 0 [8000/60000 (13%)]\tLoss: 0.181801\nTrain Epoch: 0 [9000/60000 (15%)]\tLoss: 0.294713\nTrain Epoch: 0 [10000/60000 (17%)]\tLoss: 0.217551\nTrain Epoch: 0 [11000/60000 (18%)]\tLoss: 0.326670\nTrain Epoch: 0 [12000/60000 (20%)]\tLoss: 0.769117\nTrain Epoch: 0 [13000/60000 (22%)]\tLoss: 0.381453\nTrain Epoch: 0 [14000/60000 (23%)]\tLoss: 0.729524\nTrain Epoch: 0 [15000/60000 (25%)]\tLoss: 0.682405\nTrain Epoch: 0 [16000/60000 (27%)]\tLoss: 0.181199\nTrain Epoch: 0 [17000/60000 (28%)]\tLoss: 0.097385\nTrain Epoch: 0 [18000/60000 (30%)]\tLoss: 0.191438\nTrain Epoch: 0 [19000/60000 (32%)]\tLoss: 0.401812\nTrain Epoch: 0 [20000/60000 (33%)]\tLoss: 0.232209\nTrain Epoch: 0 [21000/60000 (35%)]\tLoss: 0.536690\nTrain Epoch: 0 [22000/60000 (37%)]\tLoss: 0.507774\nTrain Epoch: 0 [23000/60000 (38%)]\tLoss: 0.206170\nTrain Epoch: 0 [24000/60000 (40%)]\tLoss: 0.089738\nTrain Epoch: 0 [25000/60000 (42%)]\tLoss: 0.240920\nTrain Epoch: 0 [26000/60000 (43%)]\tLoss: 1.025469\nTrain Epoch: 0 [27000/60000 (45%)]\tLoss: 0.277160\nTrain Epoch: 0 [28000/60000 (47%)]\tLoss: 0.178560\nTrain Epoch: 0 [29000/60000 (48%)]\tLoss: 0.009993\nTrain Epoch: 0 [30000/60000 (50%)]\tLoss: 0.109582\nTrain Epoch: 0 [31000/60000 (52%)]\tLoss: 0.571526\nTrain Epoch: 0 [32000/60000 (53%)]\tLoss: 0.120413\nTrain Epoch: 0 [33000/60000 (55%)]\tLoss: 0.602681\nTrain Epoch: 0 [34000/60000 (57%)]\tLoss: 0.183352\nTrain Epoch: 0 [35000/60000 (58%)]\tLoss: 0.042493\nTrain Epoch: 0 [36000/60000 (60%)]\tLoss: 0.398519\nTrain Epoch: 0 [37000/60000 (62%)]\tLoss: 0.082055\nTrain Epoch: 0 [38000/60000 (63%)]\tLoss: 0.539477\nTrain Epoch: 0 [39000/60000 (65%)]\tLoss: 0.061902\nTrain Epoch: 0 [40000/60000 (67%)]\tLoss: 0.269085\nTrain Epoch: 0 [41000/60000 (68%)]\tLoss: 0.395328\nTrain Epoch: 0 [42000/60000 (70%)]\tLoss: 0.042789\nTrain Epoch: 0 [43000/60000 (72%)]\tLoss: 0.512444\nTrain Epoch: 0 [44000/60000 (73%)]\tLoss: 0.012052\nTrain Epoch: 0 [45000/60000 (75%)]\tLoss: 0.354293\nTrain Epoch: 0 [46000/60000 (77%)]\tLoss: 0.145567\nTrain Epoch: 0 [47000/60000 (78%)]\tLoss: 0.009135\nTrain Epoch: 0 [48000/60000 (80%)]\tLoss: 0.064773\nTrain Epoch: 0 [49000/60000 (82%)]\tLoss: 0.092612\nTrain Epoch: 0 [50000/60000 (83%)]\tLoss: 0.101427\nTrain Epoch: 0 [51000/60000 (85%)]\tLoss: 0.180027\nTrain Epoch: 0 [52000/60000 (87%)]\tLoss: 0.123264\nTrain Epoch: 0 [53000/60000 (88%)]\tLoss: 0.168589\nTrain Epoch: 0 [54000/60000 (90%)]\tLoss: 0.312908\nTrain Epoch: 0 [55000/60000 (92%)]\tLoss: 0.235755\nTrain Epoch: 0 [56000/60000 (93%)]\tLoss: 0.563699\nTrain Epoch: 0 [57000/60000 (95%)]\tLoss: 0.013629\nTrain Epoch: 0 [58000/60000 (97%)]\tLoss: 0.412506\nTrain Epoch: 0 [59000/60000 (98%)]\tLoss: 0.878097\n33.99s\n\nTest set: Average loss: 0.1165, Accuracy: 9644/10000 (96%)\n\n"
    }
   ],
   "source": [
    "train(1)  # training 1 epoch will get you to ~93%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up your code with nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSeq, self).__init__()\n",
    "\n",
    "        # conv layers: feature extractor\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # fc layers: classifier\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = NetSeq().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.288247\nTrain Epoch: 0 [1000/60000 (2%)]\tLoss: 2.325318\nTrain Epoch: 0 [2000/60000 (3%)]\tLoss: 2.327987\nTrain Epoch: 0 [3000/60000 (5%)]\tLoss: 2.258137\nTrain Epoch: 0 [4000/60000 (7%)]\tLoss: 2.260822\nTrain Epoch: 0 [5000/60000 (8%)]\tLoss: 2.363909\nTrain Epoch: 0 [6000/60000 (10%)]\tLoss: 2.068602\nTrain Epoch: 0 [7000/60000 (12%)]\tLoss: 1.531686\nTrain Epoch: 0 [8000/60000 (13%)]\tLoss: 1.460082\nTrain Epoch: 0 [9000/60000 (15%)]\tLoss: 1.010911\nTrain Epoch: 0 [10000/60000 (17%)]\tLoss: 1.264005\nTrain Epoch: 0 [11000/60000 (18%)]\tLoss: 1.192480\nTrain Epoch: 0 [12000/60000 (20%)]\tLoss: 0.706186\nTrain Epoch: 0 [13000/60000 (22%)]\tLoss: 0.820624\nTrain Epoch: 0 [14000/60000 (23%)]\tLoss: 1.197402\nTrain Epoch: 0 [15000/60000 (25%)]\tLoss: 1.431624\nTrain Epoch: 0 [16000/60000 (27%)]\tLoss: 0.512382\nTrain Epoch: 0 [17000/60000 (28%)]\tLoss: 0.810709\nTrain Epoch: 0 [18000/60000 (30%)]\tLoss: 0.460578\nTrain Epoch: 0 [19000/60000 (32%)]\tLoss: 0.485694\nTrain Epoch: 0 [20000/60000 (33%)]\tLoss: 0.642223\nTrain Epoch: 0 [21000/60000 (35%)]\tLoss: 0.273660\nTrain Epoch: 0 [22000/60000 (37%)]\tLoss: 0.686061\nTrain Epoch: 0 [23000/60000 (38%)]\tLoss: 0.412527\nTrain Epoch: 0 [24000/60000 (40%)]\tLoss: 0.839396\nTrain Epoch: 0 [25000/60000 (42%)]\tLoss: 0.082946\nTrain Epoch: 0 [26000/60000 (43%)]\tLoss: 0.955385\nTrain Epoch: 0 [27000/60000 (45%)]\tLoss: 0.420839\nTrain Epoch: 0 [28000/60000 (47%)]\tLoss: 0.189660\nTrain Epoch: 0 [29000/60000 (48%)]\tLoss: 0.380209\nTrain Epoch: 0 [30000/60000 (50%)]\tLoss: 0.601784\nTrain Epoch: 0 [31000/60000 (52%)]\tLoss: 0.375694\nTrain Epoch: 0 [32000/60000 (53%)]\tLoss: 0.241490\nTrain Epoch: 0 [33000/60000 (55%)]\tLoss: 0.481257\nTrain Epoch: 0 [34000/60000 (57%)]\tLoss: 0.661006\nTrain Epoch: 0 [35000/60000 (58%)]\tLoss: 0.543682\nTrain Epoch: 0 [36000/60000 (60%)]\tLoss: 1.186557\nTrain Epoch: 0 [37000/60000 (62%)]\tLoss: 0.123915\nTrain Epoch: 0 [38000/60000 (63%)]\tLoss: 0.427012\nTrain Epoch: 0 [39000/60000 (65%)]\tLoss: 0.757531\nTrain Epoch: 0 [40000/60000 (67%)]\tLoss: 0.168388\nTrain Epoch: 0 [41000/60000 (68%)]\tLoss: 0.257765\nTrain Epoch: 0 [42000/60000 (70%)]\tLoss: 0.612178\nTrain Epoch: 0 [43000/60000 (72%)]\tLoss: 0.186994\nTrain Epoch: 0 [44000/60000 (73%)]\tLoss: 0.483788\nTrain Epoch: 0 [45000/60000 (75%)]\tLoss: 0.472469\nTrain Epoch: 0 [46000/60000 (77%)]\tLoss: 0.178653\nTrain Epoch: 0 [47000/60000 (78%)]\tLoss: 0.053979\nTrain Epoch: 0 [48000/60000 (80%)]\tLoss: 0.778960\nTrain Epoch: 0 [49000/60000 (82%)]\tLoss: 0.383312\nTrain Epoch: 0 [50000/60000 (83%)]\tLoss: 0.689154\nTrain Epoch: 0 [51000/60000 (85%)]\tLoss: 0.367646\nTrain Epoch: 0 [52000/60000 (87%)]\tLoss: 0.366478\nTrain Epoch: 0 [53000/60000 (88%)]\tLoss: 0.298509\nTrain Epoch: 0 [54000/60000 (90%)]\tLoss: 0.406214\nTrain Epoch: 0 [55000/60000 (92%)]\tLoss: 0.054375\nTrain Epoch: 0 [56000/60000 (93%)]\tLoss: 0.554201\nTrain Epoch: 0 [57000/60000 (95%)]\tLoss: 0.342172\nTrain Epoch: 0 [58000/60000 (97%)]\tLoss: 1.060387\nTrain Epoch: 0 [59000/60000 (98%)]\tLoss: 0.326925\n41.14s\n\nTest set: Average loss: 0.1483, Accuracy: 9558/10000 (96%)\n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 0.397651\nTrain Epoch: 1 [1000/60000 (2%)]\tLoss: 0.026102\nTrain Epoch: 1 [2000/60000 (3%)]\tLoss: 0.010735\nTrain Epoch: 1 [3000/60000 (5%)]\tLoss: 0.025344\nTrain Epoch: 1 [4000/60000 (7%)]\tLoss: 0.503073\nTrain Epoch: 1 [5000/60000 (8%)]\tLoss: 0.022485\nTrain Epoch: 1 [6000/60000 (10%)]\tLoss: 0.435582\nTrain Epoch: 1 [7000/60000 (12%)]\tLoss: 0.012186\nTrain Epoch: 1 [8000/60000 (13%)]\tLoss: 0.175691\nTrain Epoch: 1 [9000/60000 (15%)]\tLoss: 0.052222\nTrain Epoch: 1 [10000/60000 (17%)]\tLoss: 0.450759\nTrain Epoch: 1 [11000/60000 (18%)]\tLoss: 0.028507\nTrain Epoch: 1 [12000/60000 (20%)]\tLoss: 0.004463\nTrain Epoch: 1 [13000/60000 (22%)]\tLoss: 0.010768\nTrain Epoch: 1 [14000/60000 (23%)]\tLoss: 0.190738\nTrain Epoch: 1 [15000/60000 (25%)]\tLoss: 0.025781\nTrain Epoch: 1 [16000/60000 (27%)]\tLoss: 0.012361\nTrain Epoch: 1 [17000/60000 (28%)]\tLoss: 0.011768\nTrain Epoch: 1 [18000/60000 (30%)]\tLoss: 0.122256\nTrain Epoch: 1 [19000/60000 (32%)]\tLoss: 0.013686\nTrain Epoch: 1 [20000/60000 (33%)]\tLoss: 0.017120\nTrain Epoch: 1 [21000/60000 (35%)]\tLoss: 0.026584\nTrain Epoch: 1 [22000/60000 (37%)]\tLoss: 0.029468\nTrain Epoch: 1 [23000/60000 (38%)]\tLoss: 0.409214\nTrain Epoch: 1 [24000/60000 (40%)]\tLoss: 0.010499\nTrain Epoch: 1 [25000/60000 (42%)]\tLoss: 0.271650\nTrain Epoch: 1 [26000/60000 (43%)]\tLoss: 0.003558\nTrain Epoch: 1 [27000/60000 (45%)]\tLoss: 0.035003\nTrain Epoch: 1 [28000/60000 (47%)]\tLoss: 0.006983\nTrain Epoch: 1 [29000/60000 (48%)]\tLoss: 0.110231\nTrain Epoch: 1 [30000/60000 (50%)]\tLoss: 0.022056\nTrain Epoch: 1 [31000/60000 (52%)]\tLoss: 0.003076\nTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.005467\nTrain Epoch: 1 [33000/60000 (55%)]\tLoss: 0.002168\nTrain Epoch: 1 [34000/60000 (57%)]\tLoss: 0.002753\nTrain Epoch: 1 [35000/60000 (58%)]\tLoss: 0.004520\nTrain Epoch: 1 [36000/60000 (60%)]\tLoss: 0.266961\nTrain Epoch: 1 [37000/60000 (62%)]\tLoss: 0.228395\nTrain Epoch: 1 [38000/60000 (63%)]\tLoss: 0.091740\nTrain Epoch: 1 [39000/60000 (65%)]\tLoss: 0.003919\nTrain Epoch: 1 [40000/60000 (67%)]\tLoss: 0.154739\nTrain Epoch: 1 [41000/60000 (68%)]\tLoss: 0.004710\nTrain Epoch: 1 [42000/60000 (70%)]\tLoss: 0.099791\nTrain Epoch: 1 [43000/60000 (72%)]\tLoss: 0.024306\nTrain Epoch: 1 [44000/60000 (73%)]\tLoss: 0.044478\nTrain Epoch: 1 [45000/60000 (75%)]\tLoss: 0.000561\nTrain Epoch: 1 [46000/60000 (77%)]\tLoss: 0.195401\nTrain Epoch: 1 [47000/60000 (78%)]\tLoss: 0.000745\nTrain Epoch: 1 [48000/60000 (80%)]\tLoss: 0.005422\nTrain Epoch: 1 [49000/60000 (82%)]\tLoss: 0.038533\nTrain Epoch: 1 [50000/60000 (83%)]\tLoss: 0.399256\nTrain Epoch: 1 [51000/60000 (85%)]\tLoss: 0.014958\nTrain Epoch: 1 [52000/60000 (87%)]\tLoss: 0.030674\nTrain Epoch: 1 [53000/60000 (88%)]\tLoss: 0.212880\nTrain Epoch: 1 [54000/60000 (90%)]\tLoss: 0.008988\nTrain Epoch: 1 [55000/60000 (92%)]\tLoss: 0.150173\nTrain Epoch: 1 [56000/60000 (93%)]\tLoss: 0.000562\nTrain Epoch: 1 [57000/60000 (95%)]\tLoss: 0.076717\nTrain Epoch: 1 [58000/60000 (97%)]\tLoss: 0.011387\nTrain Epoch: 1 [59000/60000 (98%)]\tLoss: 0.035626\n38.78s\n\nTest set: Average loss: 0.0653, Accuracy: 9788/10000 (98%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.136250\nTrain Epoch: 2 [1000/60000 (2%)]\tLoss: 0.845388\nTrain Epoch: 2 [2000/60000 (3%)]\tLoss: 0.034080\nTrain Epoch: 2 [3000/60000 (5%)]\tLoss: 0.212885\nTrain Epoch: 2 [4000/60000 (7%)]\tLoss: 0.034633\nTrain Epoch: 2 [5000/60000 (8%)]\tLoss: 0.191599\nTrain Epoch: 2 [6000/60000 (10%)]\tLoss: 0.012651\nTrain Epoch: 2 [7000/60000 (12%)]\tLoss: 0.093949\nTrain Epoch: 2 [8000/60000 (13%)]\tLoss: 0.003123\nTrain Epoch: 2 [9000/60000 (15%)]\tLoss: 0.006282\nTrain Epoch: 2 [10000/60000 (17%)]\tLoss: 0.069466\nTrain Epoch: 2 [11000/60000 (18%)]\tLoss: 0.244419\nTrain Epoch: 2 [12000/60000 (20%)]\tLoss: 0.006091\nTrain Epoch: 2 [13000/60000 (22%)]\tLoss: 0.017709\nTrain Epoch: 2 [14000/60000 (23%)]\tLoss: 0.016343\nTrain Epoch: 2 [15000/60000 (25%)]\tLoss: 0.201194\nTrain Epoch: 2 [16000/60000 (27%)]\tLoss: 0.100712\nTrain Epoch: 2 [17000/60000 (28%)]\tLoss: 0.001201\nTrain Epoch: 2 [18000/60000 (30%)]\tLoss: 0.004469\nTrain Epoch: 2 [19000/60000 (32%)]\tLoss: 0.000819\nTrain Epoch: 2 [20000/60000 (33%)]\tLoss: 0.010332\nTrain Epoch: 2 [21000/60000 (35%)]\tLoss: 0.019220\nTrain Epoch: 2 [22000/60000 (37%)]\tLoss: 0.129489\nTrain Epoch: 2 [23000/60000 (38%)]\tLoss: 0.025485\nTrain Epoch: 2 [24000/60000 (40%)]\tLoss: 0.005270\nTrain Epoch: 2 [25000/60000 (42%)]\tLoss: 0.046870\nTrain Epoch: 2 [26000/60000 (43%)]\tLoss: 0.003407\nTrain Epoch: 2 [27000/60000 (45%)]\tLoss: 0.077679\nTrain Epoch: 2 [28000/60000 (47%)]\tLoss: 0.032308\nTrain Epoch: 2 [29000/60000 (48%)]\tLoss: 0.135935\nTrain Epoch: 2 [30000/60000 (50%)]\tLoss: 0.003856\nTrain Epoch: 2 [31000/60000 (52%)]\tLoss: 0.006493\nTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.003641\nTrain Epoch: 2 [33000/60000 (55%)]\tLoss: 0.037415\nTrain Epoch: 2 [34000/60000 (57%)]\tLoss: 0.227467\nTrain Epoch: 2 [35000/60000 (58%)]\tLoss: 0.004011\nTrain Epoch: 2 [36000/60000 (60%)]\tLoss: 0.227829\nTrain Epoch: 2 [37000/60000 (62%)]\tLoss: 0.039938\nTrain Epoch: 2 [38000/60000 (63%)]\tLoss: 0.000501\nTrain Epoch: 2 [39000/60000 (65%)]\tLoss: 0.012829\nTrain Epoch: 2 [40000/60000 (67%)]\tLoss: 0.014787\nTrain Epoch: 2 [41000/60000 (68%)]\tLoss: 0.014416\nTrain Epoch: 2 [42000/60000 (70%)]\tLoss: 0.005027\nTrain Epoch: 2 [43000/60000 (72%)]\tLoss: 0.000931\nTrain Epoch: 2 [44000/60000 (73%)]\tLoss: 0.029583\nTrain Epoch: 2 [45000/60000 (75%)]\tLoss: 0.000682\nTrain Epoch: 2 [46000/60000 (77%)]\tLoss: 0.002886\nTrain Epoch: 2 [47000/60000 (78%)]\tLoss: 0.021706\nTrain Epoch: 2 [48000/60000 (80%)]\tLoss: 0.004079\nTrain Epoch: 2 [49000/60000 (82%)]\tLoss: 0.079851\nTrain Epoch: 2 [50000/60000 (83%)]\tLoss: 0.056529\nTrain Epoch: 2 [51000/60000 (85%)]\tLoss: 0.008684\nTrain Epoch: 2 [52000/60000 (87%)]\tLoss: 0.291382\nTrain Epoch: 2 [53000/60000 (88%)]\tLoss: 0.003668\nTrain Epoch: 2 [54000/60000 (90%)]\tLoss: 0.016019\nTrain Epoch: 2 [55000/60000 (92%)]\tLoss: 0.000505\nTrain Epoch: 2 [56000/60000 (93%)]\tLoss: 0.000872\nTrain Epoch: 2 [57000/60000 (95%)]\tLoss: 0.023353\nTrain Epoch: 2 [58000/60000 (97%)]\tLoss: 0.084915\nTrain Epoch: 2 [59000/60000 (98%)]\tLoss: 0.108813\n65.18s\n\nTest set: Average loss: 0.0503, Accuracy: 9848/10000 (98%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.002362\nTrain Epoch: 3 [1000/60000 (2%)]\tLoss: 0.056829\nTrain Epoch: 3 [2000/60000 (3%)]\tLoss: 0.002977\nTrain Epoch: 3 [3000/60000 (5%)]\tLoss: 0.012962\nTrain Epoch: 3 [4000/60000 (7%)]\tLoss: 0.002383\nTrain Epoch: 3 [5000/60000 (8%)]\tLoss: 0.264201\nTrain Epoch: 3 [6000/60000 (10%)]\tLoss: 0.002741\nTrain Epoch: 3 [7000/60000 (12%)]\tLoss: 0.008777\nTrain Epoch: 3 [8000/60000 (13%)]\tLoss: 0.006555\nTrain Epoch: 3 [9000/60000 (15%)]\tLoss: 0.007144\nTrain Epoch: 3 [10000/60000 (17%)]\tLoss: 0.012815\nTrain Epoch: 3 [11000/60000 (18%)]\tLoss: 0.004628\nTrain Epoch: 3 [12000/60000 (20%)]\tLoss: 0.388902\nTrain Epoch: 3 [13000/60000 (22%)]\tLoss: 0.002284\nTrain Epoch: 3 [14000/60000 (23%)]\tLoss: 0.000089\nTrain Epoch: 3 [15000/60000 (25%)]\tLoss: 0.143163\nTrain Epoch: 3 [16000/60000 (27%)]\tLoss: 0.004708\nTrain Epoch: 3 [17000/60000 (28%)]\tLoss: 0.056187\nTrain Epoch: 3 [18000/60000 (30%)]\tLoss: 0.020925\nTrain Epoch: 3 [19000/60000 (32%)]\tLoss: 0.005102\nTrain Epoch: 3 [20000/60000 (33%)]\tLoss: 0.002934\nTrain Epoch: 3 [21000/60000 (35%)]\tLoss: 0.002345\nTrain Epoch: 3 [22000/60000 (37%)]\tLoss: 0.010084\nTrain Epoch: 3 [23000/60000 (38%)]\tLoss: 0.009309\nTrain Epoch: 3 [24000/60000 (40%)]\tLoss: 0.156604\nTrain Epoch: 3 [25000/60000 (42%)]\tLoss: 0.001036\nTrain Epoch: 3 [26000/60000 (43%)]\tLoss: 0.005390\nTrain Epoch: 3 [27000/60000 (45%)]\tLoss: 0.010711\nTrain Epoch: 3 [28000/60000 (47%)]\tLoss: 0.021396\nTrain Epoch: 3 [29000/60000 (48%)]\tLoss: 0.000594\nTrain Epoch: 3 [30000/60000 (50%)]\tLoss: 0.002066\nTrain Epoch: 3 [31000/60000 (52%)]\tLoss: 0.075884\nTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001365\nTrain Epoch: 3 [33000/60000 (55%)]\tLoss: 0.002247\nTrain Epoch: 3 [34000/60000 (57%)]\tLoss: 0.024678\nTrain Epoch: 3 [35000/60000 (58%)]\tLoss: 0.000402\nTrain Epoch: 3 [36000/60000 (60%)]\tLoss: 0.001007\nTrain Epoch: 3 [37000/60000 (62%)]\tLoss: 0.001561\nTrain Epoch: 3 [38000/60000 (63%)]\tLoss: 0.146833\nTrain Epoch: 3 [39000/60000 (65%)]\tLoss: 0.035380\nTrain Epoch: 3 [40000/60000 (67%)]\tLoss: 0.050509\nTrain Epoch: 3 [41000/60000 (68%)]\tLoss: 0.065094\nTrain Epoch: 3 [42000/60000 (70%)]\tLoss: 0.151673\nTrain Epoch: 3 [43000/60000 (72%)]\tLoss: 0.001873\nTrain Epoch: 3 [44000/60000 (73%)]\tLoss: 0.000762\nTrain Epoch: 3 [45000/60000 (75%)]\tLoss: 0.186497\nTrain Epoch: 3 [46000/60000 (77%)]\tLoss: 0.006114\nTrain Epoch: 3 [47000/60000 (78%)]\tLoss: 0.051104\nTrain Epoch: 3 [48000/60000 (80%)]\tLoss: 0.068889\nTrain Epoch: 3 [49000/60000 (82%)]\tLoss: 0.001046\nTrain Epoch: 3 [50000/60000 (83%)]\tLoss: 0.136371\nTrain Epoch: 3 [51000/60000 (85%)]\tLoss: 0.011861\nTrain Epoch: 3 [52000/60000 (87%)]\tLoss: 0.002386\nTrain Epoch: 3 [53000/60000 (88%)]\tLoss: 0.008041\nTrain Epoch: 3 [54000/60000 (90%)]\tLoss: 0.001311\nTrain Epoch: 3 [55000/60000 (92%)]\tLoss: 0.060889\nTrain Epoch: 3 [56000/60000 (93%)]\tLoss: 0.000157\nTrain Epoch: 3 [57000/60000 (95%)]\tLoss: 0.631341\nTrain Epoch: 3 [58000/60000 (97%)]\tLoss: 0.042456\nTrain Epoch: 3 [59000/60000 (98%)]\tLoss: 0.060298\n60.20s\n\nTest set: Average loss: 0.0384, Accuracy: 9884/10000 (99%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.029615\nTrain Epoch: 4 [1000/60000 (2%)]\tLoss: 0.000542\nTrain Epoch: 4 [2000/60000 (3%)]\tLoss: 0.002701\nTrain Epoch: 4 [3000/60000 (5%)]\tLoss: 0.005009\nTrain Epoch: 4 [4000/60000 (7%)]\tLoss: 0.002424\nTrain Epoch: 4 [5000/60000 (8%)]\tLoss: 0.517352\nTrain Epoch: 4 [6000/60000 (10%)]\tLoss: 0.002586\nTrain Epoch: 4 [7000/60000 (12%)]\tLoss: 0.277649\nTrain Epoch: 4 [8000/60000 (13%)]\tLoss: 0.023082\nTrain Epoch: 4 [9000/60000 (15%)]\tLoss: 0.003663\nTrain Epoch: 4 [10000/60000 (17%)]\tLoss: 0.007224\nTrain Epoch: 4 [11000/60000 (18%)]\tLoss: 0.031933\nTrain Epoch: 4 [12000/60000 (20%)]\tLoss: 0.346324\nTrain Epoch: 4 [13000/60000 (22%)]\tLoss: 0.000454\nTrain Epoch: 4 [14000/60000 (23%)]\tLoss: 0.005027\nTrain Epoch: 4 [15000/60000 (25%)]\tLoss: 0.004048\nTrain Epoch: 4 [16000/60000 (27%)]\tLoss: 0.000398\nTrain Epoch: 4 [17000/60000 (28%)]\tLoss: 0.000539\nTrain Epoch: 4 [18000/60000 (30%)]\tLoss: 0.002221\nTrain Epoch: 4 [19000/60000 (32%)]\tLoss: 0.009730\nTrain Epoch: 4 [20000/60000 (33%)]\tLoss: 0.270325\nTrain Epoch: 4 [21000/60000 (35%)]\tLoss: 0.003232\nTrain Epoch: 4 [22000/60000 (37%)]\tLoss: 0.009189\nTrain Epoch: 4 [23000/60000 (38%)]\tLoss: 0.003154\nTrain Epoch: 4 [24000/60000 (40%)]\tLoss: 0.000373\nTrain Epoch: 4 [25000/60000 (42%)]\tLoss: 0.011182\nTrain Epoch: 4 [26000/60000 (43%)]\tLoss: 0.018058\nTrain Epoch: 4 [27000/60000 (45%)]\tLoss: 0.000125\nTrain Epoch: 4 [28000/60000 (47%)]\tLoss: 0.628515\nTrain Epoch: 4 [29000/60000 (48%)]\tLoss: 0.140315\nTrain Epoch: 4 [30000/60000 (50%)]\tLoss: 0.001029\nTrain Epoch: 4 [31000/60000 (52%)]\tLoss: 0.000594\nTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000378\nTrain Epoch: 4 [33000/60000 (55%)]\tLoss: 0.359195\nTrain Epoch: 4 [34000/60000 (57%)]\tLoss: 0.003369\nTrain Epoch: 4 [35000/60000 (58%)]\tLoss: 0.001457\nTrain Epoch: 4 [36000/60000 (60%)]\tLoss: 0.000179\nTrain Epoch: 4 [37000/60000 (62%)]\tLoss: 0.000785\nTrain Epoch: 4 [38000/60000 (63%)]\tLoss: 0.000180\nTrain Epoch: 4 [39000/60000 (65%)]\tLoss: 0.049406\nTrain Epoch: 4 [40000/60000 (67%)]\tLoss: 0.150028\nTrain Epoch: 4 [41000/60000 (68%)]\tLoss: 0.002681\nTrain Epoch: 4 [42000/60000 (70%)]\tLoss: 0.004848\nTrain Epoch: 4 [43000/60000 (72%)]\tLoss: 0.120033\nTrain Epoch: 4 [44000/60000 (73%)]\tLoss: 0.018275\nTrain Epoch: 4 [45000/60000 (75%)]\tLoss: 0.001299\nTrain Epoch: 4 [46000/60000 (77%)]\tLoss: 0.001575\nTrain Epoch: 4 [47000/60000 (78%)]\tLoss: 0.008005\nTrain Epoch: 4 [48000/60000 (80%)]\tLoss: 0.003448\nTrain Epoch: 4 [49000/60000 (82%)]\tLoss: 0.008310\nTrain Epoch: 4 [50000/60000 (83%)]\tLoss: 0.004160\nTrain Epoch: 4 [51000/60000 (85%)]\tLoss: 0.002902\nTrain Epoch: 4 [52000/60000 (87%)]\tLoss: 0.010448\nTrain Epoch: 4 [53000/60000 (88%)]\tLoss: 0.146513\nTrain Epoch: 4 [54000/60000 (90%)]\tLoss: 0.089260\nTrain Epoch: 4 [55000/60000 (92%)]\tLoss: 0.000473\nTrain Epoch: 4 [56000/60000 (93%)]\tLoss: 0.001864\nTrain Epoch: 4 [57000/60000 (95%)]\tLoss: 0.017869\nTrain Epoch: 4 [58000/60000 (97%)]\tLoss: 0.000325\nTrain Epoch: 4 [59000/60000 (98%)]\tLoss: 0.029426\n52.18s\n\nTest set: Average loss: 0.0396, Accuracy: 9882/10000 (99%)\n\n"
    }
   ],
   "source": [
    "train(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}